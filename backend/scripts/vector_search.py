#!/usr/bin/env python3
"""
ç»Ÿä¸€å‘é‡æœç´¢æœåŠ¡
ä¸“æ³¨äºå‘é‡åŒ–æŠ€æœ¯å®ç°ï¼Œæä¾›ç®€åŒ–çš„æ ‡å‡†æ¥å£
- å†…å®¹é¢„å¤„ç†åœºæ™¯ï¼šæ¥æ”¶æ ‡å‡†å­—æ®µï¼Œç›´æ¥å‘é‡åŒ–å­˜å‚¨
- å¯¹è¯åœºæ™¯ï¼šæ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼Œç›´æ¥å‘é‡åŒ–æŸ¥è¯¢
"""

import sys
import os
import json
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Any
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings

class VectorSearchService:
    """ç»Ÿä¸€å‘é‡æœç´¢æœåŠ¡ - çº¯æŠ€æœ¯å®ç°"""
    
    def __init__(self, persist_directory: str = "data/chroma_db"):
        """åˆå§‹åŒ–å‘é‡æœç´¢æœåŠ¡"""
        print("ğŸ” æ­£åœ¨åˆå§‹åŒ–ç»Ÿä¸€å‘é‡æœç´¢æœåŠ¡...")
        
        self.persist_directory = persist_directory
        self.collection_name = "rss_contents_unified"
        
        # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯
        self._init_chromadb_client()
        
        # åŠ è½½å‘é‡æ¨¡å‹
        print("ğŸ“¦ æ­£åœ¨åŠ è½½sentence-transformersæ¨¡å‹...")
        self.model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
        
        print("âœ… ç»Ÿä¸€å‘é‡æœç´¢æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼")
        print("ğŸ’¡ æ”¯æŒåœºæ™¯:")
        print("   - å†…å®¹é¢„å¤„ç†ï¼šç›´æ¥å­˜å‚¨æ ‡å‡†å­—æ®µ")
        print("   - ç”¨æˆ·å¯¹è¯ï¼šç›´æ¥å‘é‡åŒ–æŸ¥è¯¢")
        print("=" * 60)
    
    def _init_chromadb_client(self):
        """åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯å’Œé›†åˆ"""
        try:
            # ç¡®ä¿æŒä¹…åŒ–ç›®å½•å­˜åœ¨
            Path(self.persist_directory).mkdir(parents=True, exist_ok=True)
            
            # åˆ›å»ºChromaDBå®¢æˆ·ç«¯
            self.client = chromadb.PersistentClient(
                path=self.persist_directory,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # è·å–æˆ–åˆ›å»ºé›†åˆ
            try:
                self.collection = self.client.get_collection(
                    name=self.collection_name
                )
                print(f"ğŸ“‚ ä½¿ç”¨ç°æœ‰ChromaDBé›†åˆ: {self.collection_name}")
            except Exception:
                # é›†åˆä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°é›†åˆ
                self.collection = self.client.create_collection(
                    name=self.collection_name,
                    metadata={"description": "RSSå†…å®¹ç»Ÿä¸€å‘é‡åŒ–å­˜å‚¨"}
                )
                print(f"ğŸ†• åˆ›å»ºæ–°ChromaDBé›†åˆ: {self.collection_name}")
                
            print(f"ğŸ—„ï¸ ChromaDBæŒä¹…åŒ–è·¯å¾„: {self.persist_directory}")
            
        except Exception as e:
            print(f"âŒ ChromaDBåˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    # ===========================================
    # å†…å®¹é¢„å¤„ç†åœºæ™¯ï¼šç›´æ¥æ¥æ”¶æ ‡å‡†å­—æ®µ
    # ===========================================
    
    def add_content_vector(self, content_id: int, title: str, summary: str, 
                          topics: str, tags: List[str], platform: str = "", 
                          publish_date: str = "", original_link: str = ""):
        """
        å†…å®¹é¢„å¤„ç†åœºæ™¯ï¼šæ·»åŠ å†…å®¹åˆ°å‘é‡æ•°æ®åº“
        
        Args:
            content_id: å†…å®¹IDï¼ˆæ•°æ®åº“ä¸»é”®ï¼‰
            title: æ ‡é¢˜
            summary: æ‘˜è¦
            topics: å•ä¸ªä¸»é¢˜å­—ç¬¦ä¸²
            tags: çº¯æ ‡ç­¾æ•°ç»„
            platform: å¹³å°
            publish_date: å‘å¸ƒæ—¥æœŸ
            original_link: åŸå§‹é“¾æ¥
        """
        try:
            # 1. ç»„è£…å‘é‡åŒ–æ–‡æœ¬ï¼ˆå››ä¸ªæ ¸å¿ƒå…ƒç´ ï¼‰
            vectorization_text = self._prepare_vectorization_text(
                title, summary, topics, tags
            )
            
            # 2. å‡†å¤‡å…ƒæ•°æ®
            metadata = {
                'content_id': content_id,
                'title': title[:100],  # ChromaDBå­—ç¬¦ä¸²é•¿åº¦é™åˆ¶
                'summary': summary[:200],
                'topics': topics[:50],
                'tags': json.dumps(tags, ensure_ascii=False)[:300],
                'platform': platform,
                'publish_date': publish_date,
                'original_link': original_link,
                'created_at': datetime.now().isoformat(),
                'vector_type': 'content'  # æ ‡è¯†å‘é‡ç±»å‹
            }
            
            # 3. ç”Ÿæˆæ–‡æ¡£ID
            doc_id = f"content_{content_id}"
            
            # 4. æ·»åŠ åˆ°ChromaDBï¼ˆè‡ªåŠ¨å‘é‡åŒ–ï¼‰
            self.collection.add(
                documents=[vectorization_text],
                metadatas=[metadata],
                ids=[doc_id]
            )
            
            print(f"âœ… å†…å®¹å‘é‡å­˜å‚¨æˆåŠŸ: {title[:30]}... (ID: {content_id})")
            
        except Exception as e:
            print(f"âŒ å†…å®¹å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            raise
    
    def _prepare_vectorization_text(self, title: str, summary: str, 
                                   topics: str, tags: List[str]) -> str:
        """
        ç»„è£…å‘é‡åŒ–æ–‡æœ¬ï¼ˆå››ä¸ªæ ¸å¿ƒå…ƒç´ ï¼‰
        
        Args:
            title: æ ‡é¢˜
            summary: æ‘˜è¦
            topics: å•ä¸ªä¸»é¢˜å­—ç¬¦ä¸²
            tags: çº¯æ ‡ç­¾æ•°ç»„
            
        Returns:
            str: ç»„åˆåçš„å‘é‡åŒ–æ–‡æœ¬
        """
        components = []
        
        # 1. æ ‡é¢˜ï¼ˆå¿…éœ€ï¼‰
        if title and title.strip():
            components.append(f"æ ‡é¢˜: {title.strip()}")
        
        # 2. æ‘˜è¦
        if summary and summary.strip():
            components.append(f"æ‘˜è¦: {summary.strip()}")
        
        # 3. ä¸»é¢˜ï¼ˆå•ä¸ªå­—ç¬¦ä¸²ï¼‰
        if topics and topics.strip() and topics != 'å…¶ä»–':
            components.append(f"ä¸»é¢˜: {topics.strip()}")
        
        # 4. æ ‡ç­¾ï¼ˆæ•°ç»„ï¼‰
        if tags and isinstance(tags, list):
            valid_tags = [tag.strip() for tag in tags if tag and isinstance(tag, str) and tag.strip()]
            if valid_tags:
                components.append(f"æ ‡ç­¾: {', '.join(valid_tags)}")
        
        vectorization_text = " | ".join(components)
        print(f"ğŸ”¤ å‘é‡åŒ–æ–‡æœ¬: {vectorization_text[:100]}...")
        
        return vectorization_text
    
    # ===========================================
    # å¯¹è¯åœºæ™¯ï¼šç”¨æˆ·è¾“å…¥å‘é‡åŒ–å’Œæ£€ç´¢
    # ===========================================
    
    def encode_user_query(self, query_text: str) -> List[float]:
        """
        å¯¹è¯åœºæ™¯ï¼šå°†ç”¨æˆ·è¾“å…¥å‘é‡åŒ–
        
        Args:
            query_text: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            List[float]: æŸ¥è¯¢å‘é‡
        """
        try:
            print(f"ğŸ”¤ ç”¨æˆ·æŸ¥è¯¢å‘é‡åŒ–: {query_text[:50]}...")
            vector = self.model.encode([query_text])[0]
            print(f"âœ… æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸ: {len(vector)}ç»´")
            return vector.tolist()
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å‘é‡åŒ–å¤±è´¥: {e}")
            raise
    
    def search_similar_content(self, query_text: str, top_k: int = 5, 
                              platform_filter: Optional[str] = None,
                              topics_filter: Optional[str] = None) -> List[Dict]:
        """
        å¯¹è¯åœºæ™¯ï¼šåŸºäºç”¨æˆ·æŸ¥è¯¢æ£€ç´¢ç›¸ä¼¼å†…å®¹
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            platform_filter: å¹³å°è¿‡æ»¤
            topics_filter: ä¸»é¢˜è¿‡æ»¤
            
        Returns:
            List[Dict]: ç›¸ä¼¼å†…å®¹åˆ—è¡¨
        """
        try:
            # æ„å»ºè¿‡æ»¤æ¡ä»¶
            where_filter = {"vector_type": "content"}  # åªæ£€ç´¢å†…å®¹å‘é‡
            
            if platform_filter:
                where_filter["platform"] = platform_filter
            if topics_filter:
                where_filter["topics"] = topics_filter
            
            # ChromaDBæŸ¥è¯¢
            results = self.collection.query(
                query_texts=[query_text],
                n_results=top_k,
                where=where_filter,
                include=['metadatas', 'documents', 'distances']
            )
            
            # å¤„ç†ç»“æœ
            formatted_results = []
            if results and results['documents'] and results['documents'][0]:
                for i, (doc, metadata, distance) in enumerate(zip(
                    results['documents'][0],
                    results['metadatas'][0], 
                    results['distances'][0]
                )):
                    # è½¬æ¢ç›¸ä¼¼åº¦
                    similarity = max(0, 1 - distance)
                    
                    formatted_results.append({
                        'content_id': metadata.get('content_id'),
                        'title': metadata.get('title', ''),
                        'summary': metadata.get('summary', ''),
                        'topics': metadata.get('topics', ''),
                        'tags': json.loads(metadata.get('tags', '[]')),
                        'platform': metadata.get('platform', ''),
                        'publish_date': metadata.get('publish_date', ''),
                        'similarity': similarity,
                        'distance': distance,
                        'vectorization_text': doc
                    })
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            formatted_results.sort(key=lambda x: x['similarity'], reverse=True)
            print(f"ğŸ” æ£€ç´¢åˆ° {len(formatted_results)} æ¡ç›¸ä¼¼å†…å®¹")
            
            return formatted_results
            
        except Exception as e:
            print(f"âŒ å†…å®¹æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    # ===========================================
    # é€šç”¨å·¥å…·æ–¹æ³•
    # ===========================================
    
    def get_vector_stats(self) -> Dict[str, Any]:
        """è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            count = self.collection.count()
            
            # è·å–æœ€è¿‘æ·»åŠ çš„å†…å®¹
            recent_results = self.collection.get(
                limit=10,
                include=['metadatas'],
                where={"vector_type": "content"}
            )
            
            recent_items = []
            platform_stats = {}
            topics_stats = {}
            
            if recent_results and recent_results['metadatas']:
                for metadata in recent_results['metadatas']:
                    # æœ€è¿‘å†…å®¹
                    recent_items.append({
                        'content_id': metadata.get('content_id'),
                        'title': metadata.get('title', 'Unknown'),
                        'topics': metadata.get('topics', ''),
                        'created_at': metadata.get('created_at', '')
                    })
                    
                    # å¹³å°ç»Ÿè®¡
                    platform = metadata.get('platform', 'unknown')
                    platform_stats[platform] = platform_stats.get(platform, 0) + 1
                    
                    # ä¸»é¢˜ç»Ÿè®¡
                    topics = metadata.get('topics', 'å…¶ä»–')
                    topics_stats[topics] = topics_stats.get(topics, 0) + 1
            
            return {
                'total_vectors': count,
                'platform_distribution': platform_stats,
                'topics_distribution': topics_stats,
                'recent_items': recent_items[:5],
                'collection_name': self.collection_name,
                'persist_directory': self.persist_directory,
                'model_name': 'paraphrase-multilingual-MiniLM-L12-v2'
            }
            
        except Exception as e:
            print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def delete_content_vector(self, content_id: int):
        """åˆ é™¤æŒ‡å®šå†…å®¹çš„å‘é‡"""
        try:
            doc_id = f"content_{content_id}"
            self.collection.delete(ids=[doc_id])
            print(f"ğŸ—‘ï¸ å·²åˆ é™¤å†…å®¹å‘é‡: content_id={content_id}")
        except Exception as e:
            print(f"âŒ åˆ é™¤å‘é‡å¤±è´¥: {e}")
            raise
    
    def reset_collection(self):
        """é‡ç½®å‘é‡é›†åˆï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        try:
            self.client.delete_collection(name=self.collection_name)
            print(f"ğŸ—‘ï¸ å·²åˆ é™¤ChromaDBé›†åˆ: {self.collection_name}")
            
            # é‡æ–°åˆ›å»ºé›†åˆ
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "RSSå†…å®¹ç»Ÿä¸€å‘é‡åŒ–å­˜å‚¨"}
            )
            print(f"ğŸ†• é‡æ–°åˆ›å»ºChromaDBé›†åˆ: {self.collection_name}")
            
        except Exception as e:
            print(f"âŒ é‡ç½®é›†åˆå¤±è´¥: {e}")


# ===========================================
# å…¨å±€å®ä¾‹å’Œä¾¿æ·å‡½æ•°
# ===========================================

# åˆ›å»ºå…¨å±€å®ä¾‹
vector_service = VectorSearchService()

def add_content_to_vectors(content_id: int, title: str, summary: str, 
                          topics: str, tags: List[str], **kwargs):
    """ä¾¿æ·å‡½æ•°ï¼šæ·»åŠ å†…å®¹åˆ°å‘é‡æ•°æ®åº“"""
    return vector_service.add_content_vector(
        content_id, title, summary, topics, tags, **kwargs
    )

def search_content_by_query(query_text: str, **kwargs):
    """ä¾¿æ·å‡½æ•°ï¼šæ ¹æ®æŸ¥è¯¢æ£€ç´¢å†…å®¹"""
    return vector_service.search_similar_content(query_text, **kwargs)


# ===========================================
# å‘½ä»¤è¡Œæµ‹è¯•å…¥å£
# ===========================================

if __name__ == "__main__":
    print("ğŸš€ ç»Ÿä¸€å‘é‡æœç´¢æœåŠ¡æµ‹è¯•")
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "stats":
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = vector_service.get_vector_stats()
            print("\nğŸ“Š å‘é‡æ•°æ®åº“ç»Ÿè®¡:")
            print(json.dumps(stats, indent=2, ensure_ascii=False))
            
        elif sys.argv[1] == "search" and len(sys.argv) > 2:
            # æœç´¢æµ‹è¯•
            query = " ".join(sys.argv[2:])
            results = vector_service.search_similar_content(query, top_k=3)
            print(f"\nğŸ” æœç´¢ç»“æœ (æŸ¥è¯¢: {query}):")
            for i, result in enumerate(results, 1):
                print(f"{i}. {result['title']} (ç›¸ä¼¼åº¦: {result['similarity']:.3f})")
                
        elif sys.argv[1] == "reset":
            # é‡ç½®é›†åˆ
            vector_service.reset_collection()
            
        else:
            print("âŒ æœªçŸ¥å‘½ä»¤")
    else:
        print("ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
        print("  python vector_search.py stats          # æ˜¾ç¤ºç»Ÿè®¡")
        print("  python vector_search.py search æŸ¥è¯¢å†…å®¹  # æœç´¢æµ‹è¯•")
        print("  python vector_search.py reset          # é‡ç½®é›†åˆ") 