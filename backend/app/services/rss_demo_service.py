#!/usr/bin/env python3
"""
RSSå†…å®¹æ‹‰å–å’Œå¤„ç†æœåŠ¡æ¼”ç¤ºç‰ˆ
å±•ç¤ºå®Œæ•´çš„RSSå¤„ç†æµç¨‹ï¼Œä¸ä¾èµ–é¢å¤–çš„ç¬¬ä¸‰æ–¹åº“
"""

import hashlib
import re
from datetime import datetime
from typing import List, Optional, Dict, Any
from urllib.parse import urlparse

import feedparser
import requests


class RSSContentDemo:
    """RSSå†…å®¹å¤„ç†æ¼”ç¤ºæœåŠ¡"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'RSS-Subscriber-Demo/1.0',
            'Accept': 'application/rss+xml, application/xml, text/xml, */*'
        }
    
    def demo_rss_processing(self, rss_url: str) -> Dict[str, Any]:
        """
        æ¼”ç¤ºRSSå†…å®¹å¤„ç†çš„å®Œæ•´æµç¨‹
        
        Args:
            rss_url: RSS URL
            
        Returns:
            Dict: å¤„ç†ç»“æœç»Ÿè®¡
        """
        print(f"\nğŸš€ å¼€å§‹RSSå†…å®¹å¤„ç†æ¼”ç¤º")
        print(f"ğŸ“¡ ç›®æ ‡URL: {rss_url}")
        print("="*60)
        
        # ç¬¬1æ­¥ï¼šHTTPè¯·æ±‚æ‹‰å–RSSåŸå§‹æ•°æ®
        print("\nç¬¬1æ­¥ï¼šHTTPè¯·æ±‚æ‹‰å–RSSåŸå§‹æ•°æ®")
        print("-" * 30)
        raw_data = self._step1_fetch_raw(rss_url)
        if not raw_data:
            return {"error": "HTTPè¯·æ±‚å¤±è´¥"}
        
        # ç¬¬2æ­¥ï¼šfeedparserè§£æRSS/Atomæ ¼å¼
        print("\nç¬¬2æ­¥ï¼šfeedparserè§£æRSS/Atomæ ¼å¼")
        print("-" * 30)
        feed_data = self._step2_parse_feed(raw_data)
        if not feed_data:
            return {"error": "RSSè§£æå¤±è´¥"}
        
        # ç¬¬3æ­¥ï¼šæå–å’Œæ¸…æ´—å†…å®¹æ•°æ®
        print("\nç¬¬3æ­¥ï¼šæå–å’Œæ¸…æ´—å†…å®¹æ•°æ®")
        print("-" * 30)
        raw_entries = self._step3_extract_entries(feed_data)
        
        # ç¬¬4æ­¥ï¼šå†…å®¹å»é‡å¤„ç†
        print("\nç¬¬4æ­¥ï¼šå†…å®¹å»é‡å¤„ç†")
        print("-" * 30)
        unique_entries = self._step4_deduplicate(raw_entries)
        
        # ç¬¬5æ­¥ï¼šæ™ºèƒ½å†…å®¹å¤„ç†
        print("\nç¬¬5æ­¥ï¼šæ™ºèƒ½å†…å®¹å¤„ç†ï¼ˆæ ‡ç­¾æå–ã€æ‘˜è¦ç”Ÿæˆï¼‰")
        print("-" * 30)
        processed_entries = self._step5_intelligent_processing(unique_entries)
        
        # ç¬¬6æ­¥ï¼šæ˜¾ç¤ºå¤„ç†ç»“æœ
        print("\nç¬¬6æ­¥ï¼šå¤„ç†ç»“æœå±•ç¤º")
        print("-" * 30)
        self._step6_display_results(processed_entries)
        
        return {
            "success": True,
            "feed_title": feed_data.feed.get('title', ''),
            "raw_count": len(feed_data.entries),
            "extracted_count": len(raw_entries),
            "unique_count": len(unique_entries),
            "processed_count": len(processed_entries)
        }
    
    def _step1_fetch_raw(self, rss_url: str) -> Optional[bytes]:
        """ç¬¬1æ­¥ï¼šHTTPè¯·æ±‚æ‹‰å–RSSåŸå§‹æ•°æ®"""
        try:
            print(f"ğŸ“¡ å‘é€HTTPè¯·æ±‚åˆ°: {rss_url}")
            response = requests.get(rss_url, headers=self.headers, timeout=15)
            
            print(f"âœ… HTTPçŠ¶æ€ç : {response.status_code}")
            print(f"ğŸ“¦ å“åº”å¤§å°: {len(response.content)} bytes")
            print(f"ğŸ“„ Content-Type: {response.headers.get('content-type', 'æœªçŸ¥')}")
            
            if response.status_code == 200:
                return response.content
            else:
                print(f"âŒ HTTPè¯·æ±‚å¤±è´¥: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ HTTPè¯·æ±‚å¼‚å¸¸: {e}")
            return None
    
    def _step2_parse_feed(self, raw_data: bytes) -> Optional[Any]:
        """ç¬¬2æ­¥ï¼šfeedparserè§£æRSS/Atomæ ¼å¼"""
        try:
            print("ğŸ” ä½¿ç”¨feedparserè§£æRSSå†…å®¹...")
            feed = feedparser.parse(raw_data)
            
            print(f"ğŸ“° Feedæ ‡é¢˜: {feed.feed.get('title', 'æ— æ ‡é¢˜')}")
            print(f"ğŸ“ Feedæè¿°: {feed.feed.get('description', 'æ— æè¿°')[:100]}...")
            print(f"ğŸ“Š è§£æåˆ°çš„æ¡ç›®æ•°: {len(feed.entries)}")
            
            if hasattr(feed, 'bozo') and feed.bozo:
                print(f"âš ï¸ RSSæ ¼å¼è­¦å‘Š: {feed.bozo_exception}")
            
            if feed.entries:
                print("âœ… RSSè§£ææˆåŠŸ")
                return feed
            else:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°RSSæ¡ç›®")
                return None
                
        except Exception as e:
            print(f"âŒ RSSè§£æå¼‚å¸¸: {e}")
            return None
    
    def _step3_extract_entries(self, feed: Any) -> List[Dict]:
        """ç¬¬3æ­¥ï¼šæå–å’Œæ¸…æ´—å†…å®¹æ•°æ®"""
        print(f"ğŸ“ å¼€å§‹æå– {len(feed.entries)} æ¡RSSæ¡ç›®...")
        
        extracted_entries = []
        
        for i, entry in enumerate(feed.entries, 1):
            try:
                # æå–åŸºç¡€å­—æ®µ
                title = self._clean_text(entry.get('title', 'æ— æ ‡é¢˜'))
                link = entry.get('link', '')
                
                # å¤„ç†æè¿°å†…å®¹
                description = self._extract_description(entry)
                
                # å¤„ç†å‘å¸ƒæ—¶é—´
                pub_date = self._parse_date(entry)
                
                # ç”Ÿæˆå†…å®¹å“ˆå¸Œ
                content_hash = self._generate_hash(title, link, description)
                
                extracted_entry = {
                    'title': title,
                    'link': link,
                    'description': description,
                    'pub_date': pub_date,
                    'content_hash': content_hash,
                    'raw_entry': entry  # ä¿ç•™åŸå§‹æ•°æ®ç”¨äºè°ƒè¯•
                }
                
                extracted_entries.append(extracted_entry)
                print(f"ğŸ“„ {i}. {title[:50]}...")
                
            except Exception as e:
                print(f"âš ï¸ æ¡ç›® {i} æå–å¤±è´¥: {e}")
                continue
        
        print(f"âœ… æˆåŠŸæå– {len(extracted_entries)} æ¡å†…å®¹")
        return extracted_entries
    
    def _step4_deduplicate(self, entries: List[Dict]) -> List[Dict]:
        """ç¬¬4æ­¥ï¼šå†…å®¹å»é‡å¤„ç†"""
        print(f"ğŸ”„ å¼€å§‹å»é‡å¤„ç†ï¼ŒåŸå§‹æ¡ç›®æ•°: {len(entries)}")
        
        seen_hashes = set()
        unique_entries = []
        
        for entry in entries:
            if entry['content_hash'] not in seen_hashes:
                seen_hashes.add(entry['content_hash'])
                unique_entries.append(entry)
            else:
                print(f"ğŸ”„ å‘ç°é‡å¤å†…å®¹: {entry['title'][:30]}...")
        
        print(f"âœ… å»é‡å®Œæˆ: {len(entries)} â†’ {len(unique_entries)} æ¡")
        return unique_entries
    
    def _step5_intelligent_processing(self, entries: List[Dict]) -> List[Dict]:
        """ç¬¬5æ­¥ï¼šæ™ºèƒ½å†…å®¹å¤„ç†"""
        print(f"ğŸ§  å¼€å§‹æ™ºèƒ½å¤„ç† {len(entries)} æ¡å†…å®¹...")
        
        # ä½¿ç”¨ContentProcessingUtilsç»Ÿä¸€å¤„ç†
        from app.services.content_processing_utils import ContentProcessingUtils
        
        processed_entries = []
        
        for entry in entries:
            try:
                # ä½¿ç”¨ContentProcessingUtilsçš„å®Œæ•´å¤„ç†
                fallback_result = ContentProcessingUtils.process_content_with_fallback(
                    title=entry['title'],
                    description=entry['description'],
                    description_text=entry['description'],  # ä½¿ç”¨ç›¸åŒçš„æè¿°
                    author="",  # demoä¸­æ²¡æœ‰ä½œè€…ä¿¡æ¯
                    platform=ContentProcessingUtils.detect_platform(entry['link']),
                    feed_title=""  # demoä¸­æ²¡æœ‰feedæ ‡é¢˜
                )
                
                # åº”ç”¨å¤„ç†ç»“æœ
                entry['smart_summary'] = fallback_result['summary']
                entry['tags'] = fallback_result['tags']  # JSONæ ¼å¼
                
                # è¯†åˆ«å¹³å°
                entry['platform'] = ContentProcessingUtils.detect_platform(entry['link'])
                
                processed_entries.append(entry)
                
                print(f"ğŸ·ï¸  æ ‡ç­¾æå–: {entry['tags']}")
                
            except Exception as e:
                print(f"âš ï¸ æ™ºèƒ½å¤„ç†å¤±è´¥: {e}")
                processed_entries.append(entry)  # ä¿ç•™åŸå§‹å†…å®¹
        
        print(f"âœ… æ™ºèƒ½å¤„ç†å®Œæˆ: {len(processed_entries)} æ¡")
        return processed_entries
    
    def _step6_display_results(self, entries: List[Dict]):
        """ç¬¬6æ­¥ï¼šæ˜¾ç¤ºå¤„ç†ç»“æœ"""
        print(f"ğŸ“‹ æœ€ç»ˆå¤„ç†ç»“æœå±•ç¤º (å‰3æ¡):")
        
        for i, entry in enumerate(entries[:3], 1):
            print(f"\n--- å†…å®¹ {i} ---")
            print(f"ğŸ“„ æ ‡é¢˜: {entry['title']}")
            print(f"ğŸ”— é“¾æ¥: {entry['link']}")
            print(f"ğŸ“ æ™ºèƒ½æ‘˜è¦: {entry['smart_summary']}")
            print(f"ğŸ·ï¸  æ ‡ç­¾: {entry['tags']}")
            print(f"ğŸ“± å¹³å°: {entry['platform']}")
            print(f"ğŸ“… å‘å¸ƒæ—¶é—´: {entry['pub_date']}")
            print(f"ğŸ”’ å†…å®¹å“ˆå¸Œ: {entry['content_hash'][:16]}...")
    
    def _clean_text(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬å†…å®¹"""
        if not text:
            return ""
        
        # ç®€å•çš„HTMLæ ‡ç­¾ç§»é™¤
        text = re.sub(r'<[^>]+>', '', text)
        
        # å»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _extract_description(self, entry: Any) -> str:
        """æå–æè¿°å†…å®¹"""
        # å°è¯•å¤šä¸ªå­—æ®µ
        for field in ['summary', 'description', 'content']:
            if hasattr(entry, field) and entry.get(field):
                desc = entry[field]
                if isinstance(desc, list) and desc:
                    desc = desc[0].get('value', '')
                
                clean_desc = self._clean_text(str(desc))
                if clean_desc:
                    return clean_desc[:300] + "..." if len(clean_desc) > 300 else clean_desc
        
        return "æ— æè¿°å†…å®¹"
    
    def _parse_date(self, entry: Any) -> str:
        """è§£æå‘å¸ƒæ—¶é—´"""
        time_fields = ['published_parsed', 'updated_parsed']
        
        for field in time_fields:
            if hasattr(entry, field) and entry.get(field):
                try:
                    time_struct = entry[field]
                    dt = datetime(*time_struct[:6])
                    return dt.strftime('%Y-%m-%d %H:%M:%S')
                except:
                    continue
        
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    def _generate_hash(self, title: str, link: str, description: str) -> str:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œ"""
        content = f"{title}{link}{description}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    



def main():
    """ä¸»æ¼”ç¤ºç¨‹åº"""
    print("ğŸ” RSSå†…å®¹æ‹‰å–å’Œå¤„ç†å®Œæ•´æµç¨‹æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºæ¼”ç¤ºæœåŠ¡
    demo = RSSContentDemo()
    
    # æµ‹è¯•URLåˆ—è¡¨
    test_urls = [
        "https://rsshub.app/bilibili/user/video/2267573",
        "https://rsshub.app/weibo/user/1195230310"
    ]
    
    for i, url in enumerate(test_urls, 1):
        print(f"\n\n{'='*80}")
        print(f"æ¼”ç¤º {i}: {url}")
        print('='*80)
        
        result = demo.demo_rss_processing(url)
        
        print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"- Feedæ ‡é¢˜: {result.get('feed_title', 'N/A')}")
        print(f"- åŸå§‹æ¡ç›®æ•°: {result.get('raw_count', 0)}")
        print(f"- æå–æ¡ç›®æ•°: {result.get('extracted_count', 0)}")
        print(f"- å»é‡åæ¡ç›®æ•°: {result.get('unique_count', 0)}")
        print(f"- æœ€ç»ˆå¤„ç†æ¡ç›®æ•°: {result.get('processed_count', 0)}")


if __name__ == "__main__":
    main() 