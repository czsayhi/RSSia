#!/usr/bin/env python3
"""
å¢å¼ºRSSå†…å®¹å¤„ç†æœåŠ¡
å®ç°æ‚¨æå‡ºçš„å®Œæ•´ä¸šåŠ¡é€»è¾‘
"""

import re
import json
from datetime import datetime
from typing import List, Optional, Dict, Any, Tuple
from urllib.parse import urlparse, urljoin
import requests
import feedparser
from bs4 import BeautifulSoup
from loguru import logger

from app.models.content import (
    RSSContentItem, FeedInfo, MediaItem, ContentType, 
    PlatformType, ContentProcessingConfig
)


class EnhancedRSSService:
    """å¢å¼ºRSSå†…å®¹å¤„ç†æœåŠ¡"""
    
    def __init__(self, config: ContentProcessingConfig = None):
        self.config = config or ContentProcessingConfig()
        self.headers = {
            'User-Agent': 'RSS-Subscriber-Bot/1.0 (RSSæ™ºèƒ½è®¢é˜…å™¨)',
            'Accept': 'application/rss+xml, application/xml, text/xml, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
        }
        # è§†é¢‘æ—¶é•¿æœåŠ¡æš‚æ—¶ç¦ç”¨ï¼Œä½¿ç”¨å†…ç½®æ–¹æ³•
        self.video_duration_service = None
    
    def fetch_and_process_content(self, rss_url: str, subscription_id: int, platform: PlatformType) -> List[RSSContentItem]:
        """
        ä¸»å¤„ç†å‡½æ•°ï¼šæ‹‰å–å¹¶å¤„ç†RSSå†…å®¹
        """
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†RSSå†…å®¹: {rss_url}")
        
        try:
            # Step 1: æ‹‰å–åŸå§‹RSSæ•°æ®
            raw_content = self._fetch_raw_rss(rss_url)
            if not raw_content:
                return []
            
            # Step 2: è§£æRSS Feed
            feed = feedparser.parse(raw_content)
            if not feed.entries:
                logger.warning("âš ï¸ RSSæºæ²¡æœ‰æ¡ç›®")
                return []
            
            # Step 3: æå–Feedçº§åˆ«ä¿¡æ¯
            feed_info = self._extract_feed_info(feed, platform)
            
            # Step 4: å¤„ç†æ¯ä¸ªæ¡ç›®
            processed_items = []
            for entry in feed.entries:
                try:
                    item = self._process_single_entry(entry, feed_info, subscription_id)
                    if item:
                        processed_items.append(item)
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†å•æ¡æ¡ç›®å¤±è´¥: {e}")
                    continue
            
            logger.success(f"âœ… RSSå¤„ç†å®Œæˆ: {len(processed_items)}æ¡å†…å®¹")
            return processed_items
            
        except Exception as e:
            logger.error(f"âŒ RSSå¤„ç†å¤±è´¥: {rss_url} | {e}")
            return []
    
    def _fetch_raw_rss(self, rss_url: str) -> Optional[bytes]:
        """æ‹‰å–RSSåŸå§‹æ•°æ®"""
        try:
            response = requests.get(rss_url, headers=self.headers, timeout=30)
            if response.status_code == 200:
                return response.content
            return None
        except Exception as e:
            logger.error(f"âŒ æ‹‰å–RSSå¤±è´¥: {e}")
            return None
    
    def _extract_feed_info(self, feed: feedparser.FeedParserDict, platform: PlatformType) -> FeedInfo:
        """
        æå–Feedçº§åˆ«ä¿¡æ¯ (è®¢é˜…æºä¿¡æ¯)
        å®ç°æ‚¨çš„ä¸šåŠ¡è§„åˆ™ï¼š
        - è®¢é˜…æºæ ‡é¢˜ã€æè¿°ã€ä¸»é¡µåœ°å€
        - æ¸…ç† 'Powered by RSSHub' æè¿°
        """
        feed_data = feed.feed
        
        # æå–åŸºç¡€ä¿¡æ¯
        title = feed_data.get('title', 'æœªçŸ¥è®¢é˜…æº')
        description = feed_data.get('description', '')
        link = feed_data.get('link', '')
        
        # æ¸…ç†æè¿°ä¸­çš„ 'Powered by RSSHub'
        if description:
            description = re.sub(r'\s*-\s*Powered by RSSHub\s*$', '', description, flags=re.IGNORECASE)
            description = description.strip()
        
        # å¤„ç†æœ€åæ›´æ–°æ—¶é—´
        last_build_date = None
        if hasattr(feed_data, 'updated_parsed') and feed_data.updated_parsed:
            last_build_date = datetime(*feed_data.updated_parsed[:6])
        
        return FeedInfo(
            feed_title=title,
            feed_description=description,
            feed_link=link,
            platform=platform,
            last_build_date=last_build_date
        )
    
    def _process_single_entry(self, entry: feedparser.util.FeedParserDict, feed_info: FeedInfo, subscription_id: int) -> Optional[RSSContentItem]:
        """
        å¤„ç†å•ä¸ªRSSæ¡ç›®
        å®ç°æ‚¨çš„ä¸šåŠ¡è§„åˆ™ï¼š
        - æå–æ ‡é¢˜ã€ä½œè€…ã€å‘å¸ƒæ—¶é—´ã€åŸæ–‡åœ°å€ç­‰
        - å¤„ç†å¯Œåª’ä½“å†…å®¹
        - åˆ¤æ–­å†…å®¹ç±»å‹
        - ä½œè€…ä¿¡æ¯å…œåº•é€»è¾‘
        """
        
        # åŸºç¡€ä¿¡æ¯æå–
        title = self._clean_text(entry.get('title', 'æ— æ ‡é¢˜'))
        original_link = entry.get('link', '')
        published_at = self._parse_publish_date(entry)
        
        # ä½œè€…ä¿¡æ¯å¤„ç† (å«å…œåº•é€»è¾‘)
        author = self._extract_author_with_fallback(entry, feed_info)
        
        # å†…å®¹æè¿°å¤„ç†
        description = self._extract_description(entry)
        description_text = self._extract_plain_text(description)
        
        # åª’ä½“å†…å®¹æå– (åŒ…å«è§†é¢‘æ—¶é•¿)
        media_items = self._extract_media_items(entry, description, original_link, feed_info.platform)
        cover_image = self._determine_cover_image(media_items, feed_info.platform)
        
        # å†…å®¹ç±»å‹åˆ¤æ–­
        content_type = self._determine_content_type(entry, media_items, feed_info.platform)
        
        # ç”Ÿæˆå†…å®¹å“ˆå¸Œ
        content_hash = self._generate_content_hash(title, original_link, description)
        
        return RSSContentItem(
            subscription_id=subscription_id,
            content_hash=content_hash,
            feed_info=feed_info,
            
            # Itemä¿¡æ¯
            title=title,
            author=author,
            published_at=published_at,
            original_link=original_link,
            
            # å†…å®¹è¯¦æƒ…
            content_type=content_type,
            description=description,
            description_text=description_text,
            
            # åª’ä½“
            media_items=media_items,
            cover_image=cover_image,
            
            # ç³»ç»Ÿå­—æ®µ
            created_at=datetime.now()
        )
    
    def _extract_author_with_fallback(self, entry: feedparser.util.FeedParserDict, feed_info: FeedInfo) -> Optional[str]:
        """
        ä½œè€…ä¿¡æ¯æå– (ç»Ÿä¸€å…œåº•é€»è¾‘)
        ä¸šåŠ¡è§„åˆ™ï¼š
        - ä¼˜å…ˆä»itemçš„authorå­—æ®µè·å–
        - æ‰¾ä¸åˆ°æ—¶ç»Ÿä¸€ç”¨è®¢é˜…æºæ ‡é¢˜å…œåº•
        """
        
        # 1. å°è¯•ä»æ¡ç›®ä¸­æå–ä½œè€…
        author = entry.get('author', '').strip()
        if author:
            return author
        
        # 2. ä½¿ç”¨è®¢é˜…æºæ ‡é¢˜å…œåº• (æ¸…ç†å¹³å°ç‰¹å®šåç¼€)
        feed_title = feed_info.feed_title
        
        # æ¸…ç†ä¸åŒå¹³å°çš„æ ‡é¢˜åç¼€
        if feed_info.platform == PlatformType.WEIBO:
            feed_title = feed_title.replace('çš„å¾®åš', '')
        elif feed_info.platform == PlatformType.BILIBILI:
            feed_title = feed_title.replace(' çš„ bilibili ç©ºé—´', '')
            feed_title = feed_title.replace('çš„bilibiliç©ºé—´', '')
        
        return feed_title.strip() if feed_title.strip() else None
    
    def _determine_content_type(self, entry: feedparser.util.FeedParserDict, media_items: List[MediaItem], platform: PlatformType) -> ContentType:
        """
        å†…å®¹ç±»å‹åˆ¤æ–­ (ç®€åŒ–ä¸º3ç§ç±»å‹)
        ä¸šåŠ¡è§„åˆ™ï¼š
        - VIDEOï¼šåŒ…å«è§†é¢‘å†…å®¹ï¼Œå¯æ’­æ”¾
        - IMAGE_TEXTï¼šæ²¡æœ‰è§†é¢‘ï¼Œæœ‰å›¾æœ‰æ–‡ï¼Œå¤–é“¾ä¹Ÿå½’ä¸ºæ–‡æœ¬  
        - TEXTï¼šæ²¡æœ‰è§†é¢‘æ²¡æœ‰å›¾ç‰‡
        """
        
        title = entry.get('title', '').lower()
        description = entry.get('description', '').lower()
        link = entry.get('link', '')
        
        # 1. è§†é¢‘ç±»å‹åˆ¤æ–­
        # Bç«™è§†é¢‘é“¾æ¥
        if platform == PlatformType.BILIBILI and ('bilibili.com/video' in link or 'bv' in title):
            return ContentType.VIDEO
        
        # åª’ä½“é¡¹ä¸­åŒ…å«è§†é¢‘
        if any(item.type == 'video' for item in media_items):
            return ContentType.VIDEO
        
        # 2. å›¾æ–‡ç±»å‹åˆ¤æ–­  
        # åª’ä½“é¡¹ä¸­åŒ…å«å›¾ç‰‡
        if any(item.type == 'image' for item in media_items):
            return ContentType.IMAGE_TEXT
        
        # 3. é»˜è®¤çº¯æ–‡æœ¬ç±»å‹
        return ContentType.TEXT
    
    def _extract_media_items(self, entry: feedparser.util.FeedParserDict, description: str, original_link: str, platform: PlatformType) -> List[MediaItem]:
        """æå–åª’ä½“é¡¹ç›® (åŒ…å«è§†é¢‘æ—¶é•¿è·å–)"""
        media_items = []
        
        # ä»descriptionä¸­æå–å›¾ç‰‡å’Œè§†é¢‘
        soup = BeautifulSoup(description, 'html.parser')
        
        # æå–å›¾ç‰‡
        for img in soup.find_all('img'):
            src = img.get('src')
            if src:
                media_items.append(MediaItem(
                    url=src,
                    type='image',
                    description=img.get('alt', '')
                ))
        
        # æå–iframeè§†é¢‘
        for iframe in soup.find_all('iframe'):
            src = iframe.get('src')
            if src:
                duration = self._get_video_duration(src, platform)
                media_items.append(MediaItem(
                    url=src,
                    type='video',
                    description='åµŒå…¥è§†é¢‘',
                    duration=duration
                ))
        
        # æå–audioæ ‡ç­¾
        for audio in soup.find_all('audio'):
            src = audio.get('src')
            if src:
                media_items.append(MediaItem(
                    url=src,
                    type='audio',
                    description='éŸ³é¢‘å†…å®¹'
                ))
        
        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯è§†é¢‘å¹³å°ï¼Œä»åŸå§‹é“¾æ¥è·å–è§†é¢‘ä¿¡æ¯
        if platform == PlatformType.BILIBILI and 'bilibili.com/video' in original_link:
            duration = self._get_video_duration(original_link, platform)
            # å¦‚æœè¿˜æ²¡æœ‰æ·»åŠ è§†é¢‘åª’ä½“é¡¹ï¼Œæ·»åŠ ä¸€ä¸ªä¸»è§†é¢‘é¡¹
            if not any(item.type == 'video' for item in media_items):
                media_items.append(MediaItem(
                    url=original_link,
                    type='video',
                    description=entry.get('title', 'è§†é¢‘'),
                    duration=duration
                ))
            else:
                # æ›´æ–°ç°æœ‰è§†é¢‘é¡¹çš„æ—¶é•¿
                for item in media_items:
                    if item.type == 'video' and not item.duration:
                        item.duration = duration
        
        return media_items[:self.config.max_media_items]
    
    def _determine_cover_image(self, media_items: List[MediaItem], platform: PlatformType) -> Optional[str]:
        """
        ç¡®å®šå°é¢å›¾ç‰‡
        å®ç°æ‚¨çš„ä¸šåŠ¡è§„åˆ™ï¼š
        - ä¼˜å…ˆä½¿ç”¨å†…å®¹ä¸­çš„ç¬¬ä¸€å¼ å›¾ç‰‡
        - æ²¡æœ‰å›¾ç‰‡æ—¶ä½¿ç”¨å¹³å°é»˜è®¤å›¾ç‰‡
        """
        
        # æŸ¥æ‰¾ç¬¬ä¸€å¼ å›¾ç‰‡
        for item in media_items:
            if item.type == 'image':
                return item.url
        
        # ä½¿ç”¨å¹³å°é»˜è®¤å›¾ç‰‡ (å‰ç«¯å¤„ç†)
        return None
    
    def _contains_external_links(self, description: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å¤–é“¾"""
        soup = BeautifulSoup(description, 'html.parser')
        links = soup.find_all('a', href=True)
        return len(links) > 0
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ""
        # æ¸…ç†HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        # æ¸…ç†å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def _extract_plain_text(self, html_content: str) -> str:
        """ä»HTMLä¸­æå–çº¯æ–‡æœ¬"""
        if not html_content:
            return ""
        soup = BeautifulSoup(html_content, 'html.parser')
        return soup.get_text().strip()
    
    def _extract_description(self, entry: feedparser.util.FeedParserDict) -> str:
        """æå–æè¿°å†…å®¹"""
        # ä¼˜å…ˆçº§ï¼šcontent > summary > description  
        if hasattr(entry, 'content') and entry.content:
            return entry.content[0].value
        elif hasattr(entry, 'summary') and entry.summary:
            return entry.summary
        elif hasattr(entry, 'description') and entry.description:
            return entry.description
        return ""
    
    def _parse_publish_date(self, entry: feedparser.util.FeedParserDict) -> datetime:
        """è§£æå‘å¸ƒæ—¶é—´"""
        if hasattr(entry, 'published_parsed') and entry.published_parsed:
            return datetime(*entry.published_parsed[:6])
        elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
            return datetime(*entry.updated_parsed[:6])
        else:
            return datetime.now()
    
    def _generate_content_hash(self, title: str, link: str, description: str) -> str:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºå»é‡"""
        import hashlib
        content = f"{title}|{link}|{description[:200]}"
        return hashlib.sha256(content.encode()).hexdigest()
    
    def _get_video_duration(self, video_url: str, platform: PlatformType) -> Optional[int]:
        """
        è·å–è§†é¢‘æ—¶é•¿ï¼ˆé›†æˆåˆ°åª’ä½“å¤„ç†æµç¨‹ä¸­ï¼‰
        
        Args:
            video_url: è§†é¢‘é“¾æ¥
            platform: å¹³å°ç±»å‹
            
        Returns:
            int: è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œè·å–å¤±è´¥è¿”å›None
        """
        try:
            if not video_url or platform != PlatformType.BILIBILI:
                return None
            
            # æå–Bç«™è§†é¢‘ID
            import re
            bv_match = re.search(r'BV([A-Za-z0-9]+)', video_url)
            av_match = re.search(r'av(\d+)', video_url)
            
            if bv_match:
                bv_id = f"BV{bv_match.group(1)}"
                api_url = f"https://api.bilibili.com/x/web-interface/view?bvid={bv_id}"
            elif av_match:
                av_id = av_match.group(1)
                api_url = f"https://api.bilibili.com/x/web-interface/view?aid={av_id}"
            else:
                return None
            
            # è°ƒç”¨Bç«™API
            response = requests.get(api_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            if data.get('code') == 0:
                video_info = data.get('data', {})
                duration = video_info.get('duration')
                
                if duration and isinstance(duration, (int, float)):
                    logger.debug(f"âœ… è·å–Bç«™è§†é¢‘æ—¶é•¿: {duration}ç§’")
                    return int(duration)
            
            return None
            
        except Exception as e:
            logger.debug(f"âš ï¸ è·å–è§†é¢‘æ—¶é•¿å¤±è´¥: {e}")
            return None
    
    def format_duration(self, seconds: Optional[int]) -> str:
        """
        æ ¼å¼åŒ–è§†é¢‘æ—¶é•¿ä¸ºå¯è¯»å­—ç¬¦ä¸²
        
        Args:
            seconds: æ—¶é•¿ç§’æ•°
            
        Returns:
            str: æ ¼å¼åŒ–çš„æ—¶é•¿å­—ç¬¦ä¸² (å¦‚: "1:23", "12:34", "1:23:45")
        """
        if seconds is None or seconds < 0:
            return "0:00"
        
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        secs = seconds % 60
        
        if hours > 0:
            return f"{hours}:{minutes:02d}:{secs:02d}"
        else:
            return f"{minutes}:{secs:02d}" 