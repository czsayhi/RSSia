#!/usr/bin/env python3
"""
å†…å®¹å»é‡æœåŠ¡
è´Ÿè´£RSSå†…å®¹çš„å»é‡æ£€æµ‹å’Œå“ˆå¸Œç”Ÿæˆ
"""

import hashlib
import re
from typing import Optional, Tuple, Dict, Any
from datetime import datetime
from loguru import logger

from ..core.database_manager import get_db_connection, get_db_transaction


class ContentDeduplicationService:
    """å†…å®¹å»é‡æœåŠ¡"""
    
    def __init__(self, db_path: str = "data/rss_subscriber.db"):
        self.db_path = db_path
        logger.info("ğŸ”§ å†…å®¹å»é‡æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def generate_content_hash(self, title: str, link: str) -> str:
        """
        ç”Ÿæˆå†…å®¹å“ˆå¸Œå€¼ç”¨äºå»é‡
        
        Args:
            title: å†…å®¹æ ‡é¢˜
            link: å†…å®¹é“¾æ¥
            
        Returns:
            str: 32ä½å“ˆå¸Œå€¼
        """
        try:
            # æ ‡å‡†åŒ–æ ‡é¢˜ï¼ˆå»é™¤å¤šä½™ç©ºç™½ã€ç»Ÿä¸€å¤§å°å†™ï¼‰
            normalized_title = self._normalize_text(title)
            
            # æ ‡å‡†åŒ–é“¾æ¥ï¼ˆå»é™¤æŸ¥è¯¢å‚æ•°ç­‰ï¼‰
            normalized_link = self._normalize_link(link)
            
            # ç”Ÿæˆå“ˆå¸Œ
            hash_content = f"{normalized_title}|{normalized_link}"
            hash_value = hashlib.sha256(hash_content.encode('utf-8')).hexdigest()[:32]
            
            logger.debug(f"ç”Ÿæˆå†…å®¹å“ˆå¸Œ: {title[:30]}... -> {hash_value}")
            return hash_value
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå†…å®¹å“ˆå¸Œå¤±è´¥: {e}")
            # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨åŸå§‹å†…å®¹ç”Ÿæˆå“ˆå¸Œ
            fallback_content = f"{title}|{link}"
            return hashlib.md5(fallback_content.encode('utf-8')).hexdigest()[:32]
    
    async def check_content_exists(self, content_hash: str) -> Optional[int]:
        """
        æ£€æŸ¥å†…å®¹æ˜¯å¦å·²å­˜åœ¨
        
        Args:
            content_hash: å†…å®¹å“ˆå¸Œå€¼
            
        Returns:
            Optional[int]: å¦‚æœå­˜åœ¨è¿”å›content_idï¼Œå¦åˆ™è¿”å›None
        """
        try:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                
                cursor.execute("""
                    SELECT id FROM shared_contents 
                    WHERE content_hash = ?
                """, (content_hash,))
                
                result = cursor.fetchone()
                if result:
                    logger.debug(f"å‘ç°é‡å¤å†…å®¹: hash={content_hash}, id={result[0]}")
                    return result[0]
                
                return None
                
        except Exception as e:
            logger.error(f"æ£€æŸ¥å†…å®¹æ˜¯å¦å­˜åœ¨å¤±è´¥: {e}")
            return None
    
    async def find_or_create_content(self, content_data: Dict[str, Any]) -> Tuple[int, bool]:
        """
        æŸ¥æ‰¾æˆ–åˆ›å»ºå†…å®¹
        
        Args:
            content_data: å†…å®¹æ•°æ®å­—å…¸
            
        Returns:
            Tuple[int, bool]: (content_id, is_new)
        """
        try:
            # ç”Ÿæˆå†…å®¹å“ˆå¸Œ
            content_hash = self.generate_content_hash(
                content_data.get('title', ''),
                content_data.get('original_link', '')
            )
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing_id = await self.check_content_exists(content_hash)
            if existing_id:
                return existing_id, False
            
            # åˆ›å»ºæ–°å†…å®¹
            content_id = await self._create_shared_content(content_data, content_hash)
            return content_id, True
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾æˆ–åˆ›å»ºå†…å®¹å¤±è´¥: {e}")
            raise
    
    async def _create_shared_content(self, content_data: Dict[str, Any], content_hash: str) -> int:
        """åˆ›å»ºæ–°çš„å…±äº«å†…å®¹"""
        try:
            with get_db_transaction() as conn:
                cursor = conn.cursor()
                
                # å‡†å¤‡æ’å…¥æ•°æ®
                insert_data = {
                    'content_hash': content_hash,
                    'title': content_data.get('title', ''),
                    'description': content_data.get('description', ''),
                    'description_text': content_data.get('description_text', ''),
                    'author': content_data.get('author', ''),
                    'published_at': content_data.get('published_at', datetime.now()),
                    'original_link': content_data.get('original_link', ''),
                    'content_type': content_data.get('content_type', 'text'),
                    'platform': content_data.get('platform', ''),
                    'guid': content_data.get('guid', ''),
                    'feed_title': content_data.get('feed_title', ''),
                    'feed_description': content_data.get('feed_description', ''),
                    'feed_link': content_data.get('feed_link', ''),
                    'feed_image_url': content_data.get('feed_image_url', ''),
                    'feed_last_build_date': content_data.get('feed_last_build_date'),
                    'cover_image': content_data.get('cover_image', ''),
                    'created_at': datetime.now(),
                    'updated_at': datetime.now()
                }
                
                # æ‰§è¡Œæ’å…¥
                cursor.execute("""
                    INSERT INTO shared_contents (
                        content_hash, title, description, description_text, author,
                        published_at, original_link, content_type, platform, guid,
                        feed_title, feed_description, feed_link, feed_image_url,
                        feed_last_build_date, cover_image, created_at, updated_at
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    insert_data['content_hash'],
                    insert_data['title'],
                    insert_data['description'],
                    insert_data['description_text'],
                    insert_data['author'],
                    insert_data['published_at'],
                    insert_data['original_link'],
                    insert_data['content_type'],
                    insert_data['platform'],
                    insert_data['guid'],
                    insert_data['feed_title'],
                    insert_data['feed_description'],
                    insert_data['feed_link'],
                    insert_data['feed_image_url'],
                    insert_data['feed_last_build_date'],
                    insert_data['cover_image'],
                    insert_data['created_at'],
                    insert_data['updated_at']
                ))
                
                content_id = cursor.lastrowid
                
                logger.info(f"åˆ›å»ºæ–°å…±äº«å†…å®¹: id={content_id}, title={insert_data['title'][:50]}...")
                return content_id
                
        except Exception as e:
            logger.error(f"åˆ›å»ºå…±äº«å†…å®¹å¤±è´¥: {e}")
            raise
    
    def _normalize_text(self, text: str) -> str:
        """æ ‡å‡†åŒ–æ–‡æœ¬å†…å®¹"""
        if not text:
            return ""
        
        # å»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        
        # å»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text).strip()
        
        # è½¬æ¢ä¸ºå°å†™ï¼ˆç”¨äºæ¯”è¾ƒï¼‰
        text = text.lower()
        
        return text
    
    def _normalize_link(self, link: str) -> str:
        """æ ‡å‡†åŒ–é“¾æ¥"""
        if not link:
            return ""
        
        # å»é™¤å¸¸è§çš„è·Ÿè¸ªå‚æ•°
        tracking_params = ['utm_source', 'utm_medium', 'utm_campaign', 'utm_content', 'utm_term']
        
        # ç®€å•å¤„ç†ï¼šå»é™¤æŸ¥è¯¢å‚æ•°ä¸­çš„è·Ÿè¸ªå‚æ•°
        if '?' in link:
            base_url, query_string = link.split('?', 1)
            
            # è¿‡æ»¤è·Ÿè¸ªå‚æ•°
            params = []
            for param in query_string.split('&'):
                if '=' in param:
                    key = param.split('=')[0]
                    if key not in tracking_params:
                        params.append(param)
            
            if params:
                link = f"{base_url}?{'&'.join(params)}"
            else:
                link = base_url
        
        return link.lower().strip()
    
    async def get_content_stats(self) -> Dict[str, Any]:
        """è·å–å†…å®¹å»é‡ç»Ÿè®¡ä¿¡æ¯"""
        try:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                
                # ç»Ÿè®¡å…±äº«å†…å®¹æ•°é‡
                cursor.execute("SELECT COUNT(*) FROM shared_contents")
                total_shared_contents = cursor.fetchone()[0]
                
                # ç»Ÿè®¡ç”¨æˆ·å…³ç³»æ•°é‡
                cursor.execute("SELECT COUNT(*) FROM user_content_relations WHERE expires_at > datetime('now')")
                active_relations = cursor.fetchone()[0]
                
                # è®¡ç®—å»é‡æ•ˆç‡
                if total_shared_contents > 0:
                    dedup_ratio = active_relations / total_shared_contents
                else:
                    dedup_ratio = 0
                
                return {
                    'total_shared_contents': total_shared_contents,
                    'active_user_relations': active_relations,
                    'average_users_per_content': round(dedup_ratio, 2),
                    'storage_efficiency': f"{(1 - 1/max(dedup_ratio, 1)) * 100:.1f}%" if dedup_ratio > 1 else "0%"
                }
                
        except Exception as e:
            logger.error(f"è·å–å†…å®¹ç»Ÿè®¡å¤±è´¥: {e}")
            return {}


# åˆ›å»ºå…¨å±€å®ä¾‹
content_dedup_service = ContentDeduplicationService() 