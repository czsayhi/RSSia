#!/usr/bin/env python3
"""
RSSå†…å®¹æ‹‰å–å’Œå¤„ç†æœåŠ¡
è´Ÿè´£RSSå†…å®¹çš„æ‹‰å–ã€è§£æã€å¤„ç†ã€å­˜å‚¨ç­‰æ ¸å¿ƒåŠŸèƒ½
å·²å‡çº§ä¸ºä½¿ç”¨æ–°çš„å…±äº«å†…å®¹å­˜å‚¨æ¶æ„
"""

import hashlib
import re
from datetime import datetime
from typing import List, Optional, Dict, Any, Tuple
from urllib.parse import urlparse

import feedparser
import requests
from bs4 import BeautifulSoup
from loguru import logger

from app.models.subscription import RSSContent
from .shared_content_service import SharedContentService


class RSSContentService:
    """RSSå†…å®¹å¤„ç†æœåŠ¡ï¼ˆå·²å‡çº§ä¸ºæ–°æ¶æ„ï¼‰"""
    
    def __init__(self, timeout: int = 15, user_agent: str = None):
        """
        åˆå§‹åŒ–RSSå†…å®¹æœåŠ¡
        
        Args:
            timeout: HTTPè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)
            user_agent: ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
        """
        self.timeout = timeout
        self.user_agent = user_agent or (
            "RSS-Subscriber-Bot/1.0 "
            "(RSSæ™ºèƒ½è®¢é˜…å™¨; https://github.com/user/rss-subscriber)"
        )
        
        # é…ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': self.user_agent,
            'Accept': 'application/rss+xml, application/xml, text/xml, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Cache-Control': 'no-cache'
        }
        
        # é›†æˆæ–°çš„å…±äº«å†…å®¹æœåŠ¡
        self.shared_service = SharedContentService()
        
        logger.info("ğŸ”§ RSSå†…å®¹æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆæ–°æ¶æ„ï¼‰")
    
    async def fetch_and_store_rss_content(
        self, 
        rss_url: str, 
        subscription_id: int, 
        user_id: int
    ) -> Dict[str, Any]:
        """
        æ‹‰å–å’Œå­˜å‚¨RSSå†…å®¹çš„ä¸»å…¥å£æ–¹æ³•ï¼ˆæ–°æ¶æ„ï¼‰
        
        Args:
            rss_url: RSSè®¢é˜…URL
            subscription_id: è®¢é˜…ID
            user_id: ç”¨æˆ·ID
            
        Returns:
            Dict: å¤„ç†ç»“æœç»Ÿè®¡
        """
        logger.info(f"ğŸš€ å¼€å§‹æ‹‰å–RSSå†…å®¹: {rss_url}, user_id={user_id}")
        
        try:
            # ç¬¬1æ­¥ï¼šå‘é€HTTPè¯·æ±‚æ‹‰å–RSSåŸå§‹æ•°æ®
            raw_content = self._fetch_raw_rss(rss_url)
            if not raw_content:
                return {'error': 'HTTPè¯·æ±‚å¤±è´¥'}
            
            # ç¬¬2æ­¥ï¼šä½¿ç”¨feedparserè§£æRSS/Atomå†…å®¹
            feed_data = self._parse_rss_feed(raw_content)
            if not feed_data:
                return {'error': 'RSSè§£æå¤±è´¥'}
            
            # ç¬¬3æ­¥ï¼šæå–å¹¶æ ‡å‡†åŒ–å†…å®¹æ•°æ®
            rss_items = self._extract_and_standardize_entries(feed_data)
            
            # ç¬¬4æ­¥ï¼šä½¿ç”¨æ–°æ¶æ„å­˜å‚¨å†…å®¹
            result = await self.shared_service.store_rss_content(
                rss_items=rss_items,
                subscription_id=subscription_id,
                user_id=user_id
            )
            
            logger.success(
                f"âœ… RSSå†…å®¹å¤„ç†å®Œæˆ: {rss_url} | "
                f"å¤„ç†{result.get('total_processed', 0)}æ¡ï¼Œ"
                f"æ–°å¢{result.get('new_content', 0)}æ¡ï¼Œ"
                f"å¤ç”¨{result.get('reused_content', 0)}æ¡"
            )
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ RSSå†…å®¹æ‹‰å–å¤±è´¥: {rss_url} | é”™è¯¯: {e}")
            return {'error': str(e)}
    
    def fetch_rss_content(self, rss_url: str, subscription_id: int) -> List[RSSContent]:
        """
        å‘åå…¼å®¹æ–¹æ³•ï¼šè¿”å›æ—§æ ¼å¼çš„RSSContentåˆ—è¡¨
        
        Args:
            rss_url: RSSè®¢é˜…URL
            subscription_id: è®¢é˜…ID
            
        Returns:
            List[RSSContent]: è§£æåçš„RSSå†…å®¹åˆ—è¡¨ï¼ˆå…¼å®¹æ ¼å¼ï¼‰
        """
        logger.info(f"ğŸš€ å¼€å§‹æ‹‰å–RSSå†…å®¹ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰: {rss_url}")
        
        try:
            # ç¬¬1æ­¥ï¼šå‘é€HTTPè¯·æ±‚æ‹‰å–RSSåŸå§‹æ•°æ®
            raw_content = self._fetch_raw_rss(rss_url)
            if not raw_content:
                return []
            
            # ç¬¬2æ­¥ï¼šä½¿ç”¨feedparserè§£æRSS/Atomå†…å®¹
            feed_data = self._parse_rss_feed(raw_content)
            if not feed_data:
                return []
            
            # ç¬¬3æ­¥ï¼šæå–å¹¶æ¸…æ´—å†…å®¹æ•°æ®ï¼ˆå…¼å®¹æ ¼å¼ï¼‰
            rss_entries = self._extract_entries_legacy(feed_data, subscription_id)
            
            # ç¬¬4æ­¥ï¼šå†…å®¹å»é‡å’ŒéªŒè¯
            unique_entries = self._deduplicate_content(rss_entries)
            
            # ç¬¬5æ­¥ï¼šæ™ºèƒ½å†…å®¹å¤„ç†ï¼ˆæ‘˜è¦ç”Ÿæˆã€æ ‡ç­¾æå–ï¼‰
            processed_entries = self._process_content_intelligence(unique_entries)
            
            logger.success(
                f"âœ… RSSå†…å®¹æ‹‰å–å®Œæˆï¼ˆå…¼å®¹æ¨¡å¼ï¼‰: {rss_url} | "
                f"åŸå§‹{len(feed_data.entries)}æ¡ â†’ å¤„ç†å{len(processed_entries)}æ¡"
            )
            
            return processed_entries
            
        except Exception as e:
            logger.error(f"âŒ RSSå†…å®¹æ‹‰å–å¤±è´¥: {rss_url} | é”™è¯¯: {e}")
            return []
    
    def _fetch_raw_rss(self, rss_url: str) -> Optional[bytes]:
        """
        ç¬¬1æ­¥ï¼šæ‹‰å–RSSåŸå§‹æ•°æ®
        
        Args:
            rss_url: RSS URL
            
        Returns:
            Optional[bytes]: RSSåŸå§‹å†…å®¹å­—èŠ‚æ•°æ®
        """
        logger.debug(f"ğŸ“¡ å‘é€HTTPè¯·æ±‚: {rss_url}")
        
        try:
            # ä¿®å¤ä¸­æ–‡ç¼–ç é—®é¢˜çš„headers
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'application/rss+xml, application/xml, text/xml, */*',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Charset': 'utf-8',
                'Cache-Control': 'no-cache'
            }
            
            response = requests.get(
                rss_url, 
                headers=headers,
                timeout=self.timeout,
                allow_redirects=True
            )
            
            # æ£€æŸ¥HTTPçŠ¶æ€ç 
            if response.status_code == 200:
                logger.debug(f"âœ… HTTPè¯·æ±‚æˆåŠŸ: {response.status_code} | å†…å®¹é•¿åº¦: {len(response.content)}")
                return response.content
            else:
                logger.warning(f"âš ï¸ HTTPè¯·æ±‚è¿”å›é200çŠ¶æ€: {response.status_code}")
                return None
                
        except requests.exceptions.Timeout:
            logger.error(f"âŒ HTTPè¯·æ±‚è¶…æ—¶: {rss_url}")
            return None
        except requests.exceptions.ConnectionError:
            logger.error(f"âŒ HTTPè¿æ¥é”™è¯¯: {rss_url}")
            return None
        except Exception as e:
            logger.error(f"âŒ HTTPè¯·æ±‚å¼‚å¸¸: {rss_url} | {e}")
            return None
    
    def _parse_rss_feed(self, raw_content: bytes) -> Optional[feedparser.FeedParserDict]:
        """
        ç¬¬2æ­¥ï¼šä½¿ç”¨feedparserè§£æRSS/Atomå†…å®¹
        
        Args:
            raw_content: RSSåŸå§‹å­—èŠ‚å†…å®¹
            
        Returns:
            Optional[FeedParserDict]: feedparserè§£æç»“æœ
        """
        logger.debug("ğŸ” å¼€å§‹è§£æRSSå†…å®¹...")
        
        try:
            # ä½¿ç”¨feedparserè§£æRSS/Atomæ ¼å¼
            feed = feedparser.parse(raw_content)
            
            # æ£€æŸ¥è§£ææ˜¯å¦æˆåŠŸ
            if hasattr(feed, 'bozo') and feed.bozo:
                logger.warning(f"âš ï¸ RSSæ ¼å¼å¯èƒ½æœ‰é—®é¢˜: {feed.bozo_exception}")
            
            # éªŒè¯feedæ˜¯å¦åŒ…å«entries
            if not hasattr(feed, 'entries') or not feed.entries:
                logger.warning("âš ï¸ RSSè§£æç»“æœä¸­æ²¡æœ‰æ¡ç›®æ•°æ®")
                return None
            
            logger.debug(f"âœ… RSSè§£ææˆåŠŸ: æ ‡é¢˜={feed.feed.get('title', 'æœªçŸ¥')}, æ¡ç›®æ•°={len(feed.entries)}")
            return feed
            
        except Exception as e:
            logger.error(f"âŒ RSSè§£æå¼‚å¸¸: {e}")
            return None
    
    def _extract_and_standardize_entries(self, feed: feedparser.FeedParserDict) -> List[Dict[str, Any]]:
        """
        ç¬¬3æ­¥ï¼šæå–å¹¶æ ‡å‡†åŒ–RSSæ¡ç›®æ•°æ®ï¼ˆæ–°æ¶æ„ï¼‰
        
        Args:
            feed: feedparserè§£æç»“æœ
            
        Returns:
            List[Dict]: æ ‡å‡†åŒ–çš„RSSå†…å®¹åˆ—è¡¨
        """
        logger.debug(f"ğŸ“ å¼€å§‹æå–RSSæ¡ç›®æ•°æ®ï¼ˆæ–°æ¶æ„ï¼‰ï¼Œå…±{len(feed.entries)}æ¡")
        
        rss_items = []
        
        # æå–Feedçº§åˆ«ä¿¡æ¯
        feed_info = {
            'feed_title': self._clean_text(feed.feed.get('title', 'æœªçŸ¥è®¢é˜…æº')),
            'feed_description': self._clean_text(feed.feed.get('description', '')),
            'feed_link': feed.feed.get('link', ''),
            'feed_image_url': self._extract_feed_image(feed),
            'feed_last_build_date': self._parse_feed_date(feed),
            'platform': self._detect_platform_from_feed(feed)
        }
        
        for entry in feed.entries:
            try:
                # æå–åŸºç¡€å­—æ®µ
                title = self._clean_text(entry.get('title', 'æ— æ ‡é¢˜'))
                original_link = entry.get('link', '')
                
                # å¤„ç†å‘å¸ƒæ—¶é—´
                published_at = self._parse_publish_date(entry)
                
                # æå–å’Œæ¸…æ´—æè¿°å†…å®¹
                description = self._extract_description(entry)
                description_text = self._clean_text(description)
                
                # ä½œè€…ä¿¡æ¯ï¼ˆå¸¦å…œåº•é€»è¾‘ï¼‰
                author = self._extract_author_with_fallback(entry, feed_info['feed_title'])
                
                # å†…å®¹ç±»å‹åˆ¤æ–­
                content_type = self._determine_content_type(entry, description)
                
                # æå–åª’ä½“é¡¹
                media_items = self._extract_media_items(entry, description)
                cover_image = self._determine_cover_image(media_items)
                
                # åˆ›å»ºæ ‡å‡†åŒ–å†…å®¹é¡¹
                rss_item = {
                    'title': title,
                    'description': description,
                    'description_text': description_text,
                    'author': author,
                    'published_at': published_at,
                    'original_link': original_link,
                    'content_type': content_type,
                    'platform': feed_info['platform'],
                    'guid': entry.get('guid', ''),
                    'cover_image': cover_image,
                    'media_items': media_items,
                    
                    # Feedçº§åˆ«ä¿¡æ¯
                    'feed_title': feed_info['feed_title'],
                    'feed_description': feed_info['feed_description'],
                    'feed_link': feed_info['feed_link'],
                    'feed_image_url': feed_info['feed_image_url'],
                    'feed_last_build_date': feed_info['feed_last_build_date']
                }
                
                rss_items.append(rss_item)
                logger.debug(f"ğŸ“„ æå–æ¡ç›®: {title[:50]}...")
                
            except Exception as e:
                logger.warning(f"âš ï¸ æ¡ç›®æå–å¤±è´¥: {e}")
                continue
        
        logger.debug(f"âœ… æ¡ç›®æå–å®Œæˆï¼ˆæ–°æ¶æ„ï¼‰: {len(rss_items)}æ¡")
        return rss_items
    
    def _extract_entries_legacy(self, feed: feedparser.FeedParserDict, subscription_id: int) -> List[RSSContent]:
        """
        ç¬¬3æ­¥ï¼šæå–å¹¶æ¸…æ´—RSSæ¡ç›®æ•°æ®ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
        
        Args:
            feed: feedparserè§£æç»“æœ
            subscription_id: è®¢é˜…ID
            
        Returns:
            List[RSSContent]: æå–çš„RSSå†…å®¹åˆ—è¡¨
        """
        logger.debug(f"ğŸ“ å¼€å§‹æå–RSSæ¡ç›®æ•°æ®ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰ï¼Œå…±{len(feed.entries)}æ¡")
        
        rss_entries = []
        
        for entry in feed.entries:
            try:
                # æå–åŸºç¡€å­—æ®µ
                title = self._clean_text(entry.get('title', 'æ— æ ‡é¢˜'))
                link = entry.get('link', '')
                
                # å¤„ç†å‘å¸ƒæ—¶é—´
                pub_date = self._parse_publish_date(entry)
                
                # æå–å’Œæ¸…æ´—æè¿°å†…å®¹
                description = self._extract_description(entry)
                
                # ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºå»é‡
                content_hash = self._generate_content_hash(title, link, description)
                
                # åˆ›å»ºRSSContentå¯¹è±¡
                rss_content = RSSContent(
                    subscription_id=subscription_id,
                    title=title,
                    link=link,
                    description=description,
                    pub_date=pub_date,
                    content_hash=content_hash,
                    is_read=False,
                    created_at=datetime.now()
                )
                
                rss_entries.append(rss_content)
                logger.debug(f"ğŸ“„ æå–æ¡ç›®: {title[:50]}...")
                
            except Exception as e:
                logger.warning(f"âš ï¸ æ¡ç›®æå–å¤±è´¥: {e}")
                continue
        
        logger.debug(f"âœ… æ¡ç›®æå–å®Œæˆï¼ˆå…¼å®¹æ¨¡å¼ï¼‰: {len(rss_entries)}æ¡")
        return rss_entries
    
    def _extract_author_with_fallback(self, entry: feedparser.util.FeedParserDict, feed_title: str) -> Optional[str]:
        """
        ä½œè€…ä¿¡æ¯æå–ï¼ˆå¸¦å…œåº•é€»è¾‘ï¼‰
        
        Args:
            entry: feedparseræ¡ç›®
            feed_title: è®¢é˜…æºæ ‡é¢˜
            
        Returns:
            Optional[str]: ä½œè€…ä¿¡æ¯
        """
        # 1. å°è¯•ä»æ¡ç›®ä¸­æå–ä½œè€…
        author = entry.get('author', '').strip()
        if author:
            return author
        
        # 2. ä½¿ç”¨è®¢é˜…æºæ ‡é¢˜å…œåº•ï¼ˆæ¸…ç†å¹³å°ç‰¹å®šåç¼€ï¼‰
        if feed_title:
            # æ¸…ç†ä¸åŒå¹³å°çš„æ ‡é¢˜åç¼€
            clean_title = feed_title
            suffixes_to_remove = [
                'çš„å¾®åš', ' çš„ bilibili ç©ºé—´', 'çš„bilibiliç©ºé—´',
                ' - çŸ¥ä¹', 'çš„çŸ¥ä¹ä¸“æ ', ' | å°‘æ•°æ´¾'
            ]
            
            for suffix in suffixes_to_remove:
                clean_title = clean_title.replace(suffix, '')
            
            return clean_title.strip() if clean_title.strip() else None
        
        return None
    
    def _determine_content_type(self, entry: feedparser.util.FeedParserDict, description: str) -> str:
        """
        åˆ¤æ–­å†…å®¹ç±»å‹
        
        Args:
            entry: feedparseræ¡ç›®
            description: æè¿°å†…å®¹
            
        Returns:
            str: å†…å®¹ç±»å‹
        """
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è§†é¢‘
        if any(keyword in description.lower() for keyword in ['video', 'è§†é¢‘', 'bilibili.com/video']):
            return 'video'
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
        if any(keyword in description.lower() for keyword in ['<img', 'image', 'å›¾ç‰‡']):
            return 'image_text'
        
        return 'text'
    
    def _extract_media_items(self, entry: feedparser.util.FeedParserDict, description: str) -> List[Dict[str, Any]]:
        """
        æå–åª’ä½“é¡¹
        
        Args:
            entry: feedparseræ¡ç›®
            description: æè¿°å†…å®¹
            
        Returns:
            List[Dict]: åª’ä½“é¡¹åˆ—è¡¨
        """
        media_items = []
        
        try:
            # ä»æè¿°ä¸­æå–å›¾ç‰‡
            soup = BeautifulSoup(description, 'html.parser')
            images = soup.find_all('img')
            
            for img in images:
                src = img.get('src', '')
                if src:
                    media_items.append({
                        'url': src,
                        'type': 'image',
                        'description': img.get('alt', ''),
                        'duration': None
                    })
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è§†é¢‘é“¾æ¥
            if 'bilibili.com/video' in description:
                # ç®€å•çš„è§†é¢‘æ£€æµ‹
                media_items.append({
                    'url': entry.get('link', ''),
                    'type': 'video',
                    'description': 'è§†é¢‘å†…å®¹',
                    'duration': None  # é¢„ç•™å­—æ®µ
                })
        
        except Exception as e:
            logger.debug(f"åª’ä½“é¡¹æå–å¤±è´¥: {e}")
        
        return media_items
    
    def _determine_cover_image(self, media_items: List[Dict[str, Any]]) -> Optional[str]:
        """
        ç¡®å®šå°é¢å›¾ç‰‡
        
        Args:
            media_items: åª’ä½“é¡¹åˆ—è¡¨
            
        Returns:
            Optional[str]: å°é¢å›¾ç‰‡URL
        """
        for item in media_items:
            if item.get('type') == 'image' and item.get('url'):
                return item['url']
        return None
    
    def _extract_feed_image(self, feed: feedparser.FeedParserDict) -> Optional[str]:
        """æå–Feedå›¾åƒ"""
        if hasattr(feed.feed, 'image') and feed.feed.image:
            return feed.feed.image.get('href', '')
        return None
    
    def _parse_feed_date(self, feed: feedparser.FeedParserDict) -> Optional[datetime]:
        """è§£æFeedæ„å»ºæ—¶é—´"""
        if hasattr(feed.feed, 'updated_parsed') and feed.feed.updated_parsed:
            try:
                return datetime(*feed.feed.updated_parsed[:6])
            except (ValueError, TypeError):
                pass
        return None
    
    def _detect_platform_from_feed(self, feed: feedparser.FeedParserDict) -> str:
        """ä»Feedä¿¡æ¯æ£€æµ‹å¹³å°"""
        feed_link = feed.feed.get('link', '')
        feed_title = feed.feed.get('title', '')
        
        if 'bilibili' in feed_link.lower() or 'bilibili' in feed_title.lower():
            return 'bilibili'
        elif 'weibo' in feed_link.lower() or 'å¾®åš' in feed_title:
            return 'weibo'
        elif 'github' in feed_link.lower():
            return 'github'
        elif 'zhihu' in feed_link.lower() or 'çŸ¥ä¹' in feed_title:
            return 'zhihu'
        
        return 'other'
    
    def _clean_text(self, text: str) -> str:
        """
        æ¸…æ´—æ–‡æœ¬å†…å®¹ï¼šå»é™¤HTMLæ ‡ç­¾ã€å¤šä½™ç©ºç™½å­—ç¬¦ç­‰
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            str: æ¸…æ´—åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # ä½¿ç”¨BeautifulSoupå»é™¤HTMLæ ‡ç­¾
        clean_text = BeautifulSoup(text, 'html.parser').get_text()
        
        # å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        
        return clean_text
    
    def _parse_publish_date(self, entry: feedparser.util.FeedParserDict) -> datetime:
        """
        è§£æå‘å¸ƒæ—¶é—´
        
        Args:
            entry: feedparseræ¡ç›®
            
        Returns:
            datetime: è§£æåçš„æ—¶é—´å¯¹è±¡
        """
        # å°è¯•ä»å¤šä¸ªå­—æ®µè·å–æ—¶é—´
        time_fields = ['published_parsed', 'updated_parsed', 'created_parsed']
        
        for field in time_fields:
            if hasattr(entry, field) and entry.get(field):
                try:
                    time_struct = entry[field]
                    return datetime(*time_struct[:6])
                except (ValueError, TypeError):
                    continue
        
        # å¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
        logger.debug("âš ï¸ æ— æ³•è§£æå‘å¸ƒæ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´")
        return datetime.now()
    
    def _extract_description(self, entry: feedparser.util.FeedParserDict) -> str:
        """
        æå–å’Œå¤„ç†æè¿°å†…å®¹
        
        Args:
            entry: feedparseræ¡ç›®
            
        Returns:
            str: å¤„ç†åçš„æè¿°å†…å®¹
        """
        # å°è¯•ä»å¤šä¸ªå­—æ®µè·å–æè¿°
        desc_fields = ['summary', 'description', 'content']
        
        for field in desc_fields:
            if hasattr(entry, field) and entry.get(field):
                description = entry[field]
                
                # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼ï¼ˆå¦‚contentå­—æ®µï¼‰ï¼Œå–ç¬¬ä¸€ä¸ª
                if isinstance(description, list) and description:
                    description = description[0].get('value', '')
                
                if description:
                    return description
        
        return "æ— æè¿°å†…å®¹"
    
    def _generate_content_hash(self, title: str, link: str, description: str) -> str:
        """
        ç”Ÿæˆå†…å®¹å“ˆå¸Œå€¼ç”¨äºå»é‡
        
        Args:
            title: æ ‡é¢˜
            link: é“¾æ¥
            description: æè¿°
            
        Returns:
            str: MD5å“ˆå¸Œå€¼
        """
        content = f"{title}{link}{description}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def _deduplicate_content(self, entries: List[RSSContent]) -> List[RSSContent]:
        """
        ç¬¬4æ­¥ï¼šå†…å®¹å»é‡
        
        Args:
            entries: RSSå†…å®¹åˆ—è¡¨
            
        Returns:
            List[RSSContent]: å»é‡åçš„å†…å®¹åˆ—è¡¨
        """
        seen_hashes = set()
        unique_entries = []
        
        for entry in entries:
            if entry.content_hash not in seen_hashes:
                seen_hashes.add(entry.content_hash)
                unique_entries.append(entry)
            else:
                logger.debug(f"ğŸ”„ é‡å¤å†…å®¹å·²è¿‡æ»¤: {entry.title[:50]}...")
        
        logger.debug(f"âœ… å»é‡å®Œæˆ: {len(entries)}æ¡ â†’ {len(unique_entries)}æ¡")
        return unique_entries
    
    def _process_content_intelligence(self, entries: List[RSSContent]) -> List[RSSContent]:
        """
        ç¬¬5æ­¥ï¼šæ™ºèƒ½å†…å®¹å¤„ç†ï¼ˆæ‘˜è¦ç”Ÿæˆã€æ ‡ç­¾æå–ï¼‰
        
        Args:
            entries: RSSå†…å®¹åˆ—è¡¨
            
        Returns:
            List[RSSContent]: æ™ºèƒ½å¤„ç†åçš„å†…å®¹åˆ—è¡¨
        """
        logger.debug(f"ğŸ§  å¼€å§‹æ™ºèƒ½å†…å®¹å¤„ç†ï¼Œå…±{len(entries)}æ¡")
        
        processed_entries = []
        
        for entry in entries:
            try:
                # ç”Ÿæˆæ™ºèƒ½æ‘˜è¦ï¼ˆç›®å‰ä½¿ç”¨è§„åˆ™ï¼Œæœªæ¥å¯æ¥å…¥LLMï¼‰
                entry.smart_summary = self._generate_summary(entry.title, entry.description)
                
                # æå–æ™ºèƒ½æ ‡ç­¾ï¼ˆç›®å‰ä½¿ç”¨è§„åˆ™ï¼Œæœªæ¥å¯æ¥å…¥LLMï¼‰
                entry.tags = self._extract_tags(entry.title, entry.description)
                
                # è¯†åˆ«å¹³å°ä¿¡æ¯
                entry.platform = self._detect_platform(entry.link)
                
                processed_entries.append(entry)
                
            except Exception as e:
                logger.warning(f"âš ï¸ æ™ºèƒ½å¤„ç†å¤±è´¥: {e}")
                # å³ä½¿æ™ºèƒ½å¤„ç†å¤±è´¥ï¼Œä¹Ÿä¿ç•™åŸå§‹å†…å®¹
                processed_entries.append(entry)
        
        logger.debug(f"âœ… æ™ºèƒ½å¤„ç†å®Œæˆ: {len(processed_entries)}æ¡")
        return processed_entries
    
    def _generate_summary(self, title: str, description: str) -> str:
        """
        ç”Ÿæˆæ™ºèƒ½æ‘˜è¦ï¼ˆç®€åŒ–ç‰ˆè§„åˆ™ï¼Œæœªæ¥å¯æ¥å…¥LLMï¼‰
        
        Args:
            title: æ ‡é¢˜
            description: æè¿°
            
        Returns:
            str: ç”Ÿæˆçš„æ‘˜è¦
        """
        # ç®€åŒ–æ‘˜è¦ç”Ÿæˆé€»è¾‘
        if len(description) <= 100:
            return description
        
        # å–å‰80ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦
        summary = description[:80].rstrip()
        
        # é¿å…åœ¨å¥å­ä¸­é—´æˆªæ–­
        if not summary.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
            last_punct = max(
                summary.rfind('ã€‚'), summary.rfind('ï¼'), 
                summary.rfind('ï¼Ÿ'), summary.rfind('.')
            )
            if last_punct > 30:  # ç¡®ä¿æ‘˜è¦ä¸ä¼šå¤ªçŸ­
                summary = summary[:last_punct + 1]
        
        return summary + "..."
    
    def _extract_tags(self, title: str, description: str) -> List[str]:
        """
        æå–æ™ºèƒ½æ ‡ç­¾ï¼ˆç®€åŒ–ç‰ˆè§„åˆ™ï¼Œæœªæ¥å¯æ¥å…¥LLMï¼‰
        
        Args:
            title: æ ‡é¢˜
            description: æè¿°
            
        Returns:
            List[str]: æ ‡ç­¾åˆ—è¡¨
        """
        tags = []
        content = f"{title} {description}".lower()
        
        # æŠ€æœ¯ç›¸å…³æ ‡ç­¾
        tech_keywords = {
            'python': 'Python',
            'javascript': 'JavaScript', 
            'react': 'React',
            'vue': 'Vue',
            'docker': 'Docker',
            'kubernetes': 'Kubernetes',
            'ai': 'AI',
            'äººå·¥æ™ºèƒ½': 'AI',
            'æœºå™¨å­¦ä¹ ': 'æœºå™¨å­¦ä¹ ',
            'æ·±åº¦å­¦ä¹ ': 'æ·±åº¦å­¦ä¹ '
        }
        
        for keyword, tag in tech_keywords.items():
            if keyword in content:
                tags.append(tag)
        
        # å¹³å°ç›¸å…³æ ‡ç­¾
        if 'bilibili' in content or 'å“”å“©å“”å“©' in content:
            tags.append('Bç«™')
        if 'github' in content:
            tags.append('GitHub')
        if 'weibo' in content or 'å¾®åš' in content:
            tags.append('å¾®åš')
        
        return list(set(tags))  # å»é‡
    
    def _detect_platform(self, link: str) -> str:
        """
        ä»é“¾æ¥æ£€æµ‹å¹³å°ä¿¡æ¯
        
        Args:
            link: å†…å®¹é“¾æ¥
            
        Returns:
            str: å¹³å°åç§°
        """
        if not link:
            return "unknown"
        
        domain = urlparse(link).netloc.lower()
        
        platform_mapping = {
            'bilibili.com': 'bilibili',
            'weibo.com': 'weibo', 
            'github.com': 'github',
            'juejin.cn': 'juejin',
            'zhihu.com': 'zhihu',
            'v2ex.com': 'v2ex'
        }
        
        for domain_key, platform in platform_mapping.items():
            if domain_key in domain:
                return platform
        
        return "other"


# åˆ›å»ºå…¨å±€å®ä¾‹
rss_content_service = RSSContentService()


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•æ–¹æ³•
async def example_usage():
    """RSSå†…å®¹æœåŠ¡ä½¿ç”¨ç¤ºä¾‹ï¼ˆæ–°æ¶æ„ï¼‰"""
    
    # åˆå§‹åŒ–æœåŠ¡
    rss_service = RSSContentService()
    
    # æµ‹è¯•RSS URL
    test_urls = [
        "https://rsshub.app/bilibili/user/video/2267573",  # Bç«™è§†é¢‘
        "https://rsshub.app/weibo/user/1195230310",        # å¾®åšåŠ¨æ€
    ]
    
    for i, rss_url in enumerate(test_urls, 1):
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•RSSæ‹‰å– {i}: {rss_url}")
        print('='*60)
        
        # ä½¿ç”¨æ–°æ¶æ„æ‹‰å–å’Œå­˜å‚¨RSSå†…å®¹
        result = await rss_service.fetch_and_store_rss_content(
            rss_url=rss_url, 
            subscription_id=i, 
            user_id=1
        )
        
        # æ˜¾ç¤ºç»“æœ
        print(f"å¤„ç†ç»“æœ: {result}")


if __name__ == "__main__":
    import asyncio
    asyncio.run(example_usage()) 