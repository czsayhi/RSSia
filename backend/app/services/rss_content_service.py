#!/usr/bin/env python3
"""
RSSå†…å®¹æ‹‰å–å’Œå¤„ç†æœåŠ¡
è´Ÿè´£RSSå†…å®¹çš„æ‹‰å–ã€è§£æã€å¤„ç†ã€å­˜å‚¨ç­‰æ ¸å¿ƒåŠŸèƒ½
"""

import hashlib
import re
from datetime import datetime
from typing import List, Optional, Dict, Any, Tuple
from urllib.parse import urlparse

import feedparser
import requests
from bs4 import BeautifulSoup
from loguru import logger

from app.models.subscription import RSSContent


class RSSContentService:
    """RSSå†…å®¹å¤„ç†æœåŠ¡"""
    
    def __init__(self, timeout: int = 15, user_agent: str = None):
        """
        åˆå§‹åŒ–RSSå†…å®¹æœåŠ¡
        
        Args:
            timeout: HTTPè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)
            user_agent: ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
        """
        self.timeout = timeout
        self.user_agent = user_agent or (
            "RSS-Subscriber-Bot/1.0 "
            "(RSSæ™ºèƒ½è®¢é˜…å™¨; https://github.com/user/rss-subscriber)"
        )
        
        # é…ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': self.user_agent,
            'Accept': 'application/rss+xml, application/xml, text/xml, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Cache-Control': 'no-cache'
        }
        
        logger.info("ğŸ”§ RSSå†…å®¹æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def fetch_rss_content(self, rss_url: str, subscription_id: int) -> List[RSSContent]:
        """
        æ‹‰å–å’Œè§£æRSSå†…å®¹çš„ä¸»å…¥å£æ–¹æ³•
        
        Args:
            rss_url: RSSè®¢é˜…URL
            subscription_id: è®¢é˜…ID
            
        Returns:
            List[RSSContent]: è§£æåçš„RSSå†…å®¹åˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¼€å§‹æ‹‰å–RSSå†…å®¹: {rss_url}")
        
        try:
            # ç¬¬1æ­¥ï¼šå‘é€HTTPè¯·æ±‚æ‹‰å–RSSåŸå§‹æ•°æ®
            raw_content = self._fetch_raw_rss(rss_url)
            if not raw_content:
                return []
            
            # ç¬¬2æ­¥ï¼šä½¿ç”¨feedparserè§£æRSS/Atomå†…å®¹
            feed_data = self._parse_rss_feed(raw_content)
            if not feed_data:
                return []
            
            # ç¬¬3æ­¥ï¼šæå–å¹¶æ¸…æ´—å†…å®¹æ•°æ®
            rss_entries = self._extract_entries(feed_data, subscription_id)
            
            # ç¬¬4æ­¥ï¼šå†…å®¹å»é‡å’ŒéªŒè¯
            unique_entries = self._deduplicate_content(rss_entries)
            
            # ç¬¬5æ­¥ï¼šæ™ºèƒ½å†…å®¹å¤„ç†ï¼ˆæ‘˜è¦ç”Ÿæˆã€æ ‡ç­¾æå–ï¼‰
            processed_entries = self._process_content_intelligence(unique_entries)
            
            logger.success(
                f"âœ… RSSå†…å®¹æ‹‰å–å®Œæˆ: {rss_url} | "
                f"åŸå§‹{len(feed_data.entries)}æ¡ â†’ å¤„ç†å{len(processed_entries)}æ¡"
            )
            
            return processed_entries
            
        except Exception as e:
            logger.error(f"âŒ RSSå†…å®¹æ‹‰å–å¤±è´¥: {rss_url} | é”™è¯¯: {e}")
            return []
    
    def _fetch_raw_rss(self, rss_url: str) -> Optional[bytes]:
        """
        ç¬¬1æ­¥ï¼šæ‹‰å–RSSåŸå§‹æ•°æ®
        
        Args:
            rss_url: RSS URL
            
        Returns:
            Optional[bytes]: RSSåŸå§‹å†…å®¹å­—èŠ‚æ•°æ®
        """
        logger.debug(f"ğŸ“¡ å‘é€HTTPè¯·æ±‚: {rss_url}")
        
        try:
            response = requests.get(
                rss_url, 
                headers=self.headers,
                timeout=self.timeout,
                allow_redirects=True
            )
            
            # æ£€æŸ¥HTTPçŠ¶æ€ç 
            if response.status_code == 200:
                logger.debug(f"âœ… HTTPè¯·æ±‚æˆåŠŸ: {response.status_code} | å†…å®¹é•¿åº¦: {len(response.content)}")
                return response.content
            else:
                logger.warning(f"âš ï¸ HTTPè¯·æ±‚è¿”å›é200çŠ¶æ€: {response.status_code}")
                return None
                
        except requests.exceptions.Timeout:
            logger.error(f"âŒ HTTPè¯·æ±‚è¶…æ—¶: {rss_url}")
            return None
        except requests.exceptions.ConnectionError:
            logger.error(f"âŒ HTTPè¿æ¥é”™è¯¯: {rss_url}")
            return None
        except Exception as e:
            logger.error(f"âŒ HTTPè¯·æ±‚å¼‚å¸¸: {rss_url} | {e}")
            return None
    
    def _parse_rss_feed(self, raw_content: bytes) -> Optional[feedparser.FeedParserDict]:
        """
        ç¬¬2æ­¥ï¼šä½¿ç”¨feedparserè§£æRSS/Atomå†…å®¹
        
        Args:
            raw_content: RSSåŸå§‹å­—èŠ‚å†…å®¹
            
        Returns:
            Optional[FeedParserDict]: feedparserè§£æç»“æœ
        """
        logger.debug("ğŸ” å¼€å§‹è§£æRSSå†…å®¹...")
        
        try:
            # ä½¿ç”¨feedparserè§£æRSS/Atomæ ¼å¼
            feed = feedparser.parse(raw_content)
            
            # æ£€æŸ¥è§£ææ˜¯å¦æˆåŠŸ
            if hasattr(feed, 'bozo') and feed.bozo:
                logger.warning(f"âš ï¸ RSSæ ¼å¼å¯èƒ½æœ‰é—®é¢˜: {feed.bozo_exception}")
            
            # éªŒè¯feedæ˜¯å¦åŒ…å«entries
            if not hasattr(feed, 'entries') or not feed.entries:
                logger.warning("âš ï¸ RSSè§£æç»“æœä¸­æ²¡æœ‰æ¡ç›®æ•°æ®")
                return None
            
            logger.debug(f"âœ… RSSè§£ææˆåŠŸ: æ ‡é¢˜={feed.feed.get('title', 'æœªçŸ¥')}, æ¡ç›®æ•°={len(feed.entries)}")
            return feed
            
        except Exception as e:
            logger.error(f"âŒ RSSè§£æå¼‚å¸¸: {e}")
            return None
    
    def _extract_entries(self, feed: feedparser.FeedParserDict, subscription_id: int) -> List[RSSContent]:
        """
        ç¬¬3æ­¥ï¼šæå–å¹¶æ¸…æ´—RSSæ¡ç›®æ•°æ®
        
        Args:
            feed: feedparserè§£æç»“æœ
            subscription_id: è®¢é˜…ID
            
        Returns:
            List[RSSContent]: æå–çš„RSSå†…å®¹åˆ—è¡¨
        """
        logger.debug(f"ğŸ“ å¼€å§‹æå–RSSæ¡ç›®æ•°æ®ï¼Œå…±{len(feed.entries)}æ¡")
        
        rss_entries = []
        
        for entry in feed.entries:
            try:
                # æå–åŸºç¡€å­—æ®µ
                title = self._clean_text(entry.get('title', 'æ— æ ‡é¢˜'))
                link = entry.get('link', '')
                
                # å¤„ç†å‘å¸ƒæ—¶é—´
                pub_date = self._parse_publish_date(entry)
                
                # æå–å’Œæ¸…æ´—æè¿°å†…å®¹
                description = self._extract_description(entry)
                
                # ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºå»é‡
                content_hash = self._generate_content_hash(title, link, description)
                
                # åˆ›å»ºRSSContentå¯¹è±¡
                rss_content = RSSContent(
                    subscription_id=subscription_id,
                    title=title,
                    link=link,
                    description=description,
                    pub_date=pub_date,
                    content_hash=content_hash,
                    is_read=False,
                    created_at=datetime.now()
                )
                
                rss_entries.append(rss_content)
                logger.debug(f"ğŸ“„ æå–æ¡ç›®: {title[:50]}...")
                
            except Exception as e:
                logger.warning(f"âš ï¸ æ¡ç›®æå–å¤±è´¥: {e}")
                continue
        
        logger.debug(f"âœ… æ¡ç›®æå–å®Œæˆ: {len(rss_entries)}æ¡")
        return rss_entries
    
    def _clean_text(self, text: str) -> str:
        """
        æ¸…æ´—æ–‡æœ¬å†…å®¹ï¼šå»é™¤HTMLæ ‡ç­¾ã€å¤šä½™ç©ºç™½å­—ç¬¦ç­‰
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            str: æ¸…æ´—åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # ä½¿ç”¨BeautifulSoupå»é™¤HTMLæ ‡ç­¾
        clean_text = BeautifulSoup(text, 'html.parser').get_text()
        
        # å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        
        return clean_text
    
    def _parse_publish_date(self, entry: feedparser.util.FeedParserDict) -> datetime:
        """
        è§£æå‘å¸ƒæ—¶é—´
        
        Args:
            entry: feedparseræ¡ç›®
            
        Returns:
            datetime: è§£æåçš„æ—¶é—´å¯¹è±¡
        """
        # å°è¯•ä»å¤šä¸ªå­—æ®µè·å–æ—¶é—´
        time_fields = ['published_parsed', 'updated_parsed', 'created_parsed']
        
        for field in time_fields:
            if hasattr(entry, field) and entry.get(field):
                try:
                    time_struct = entry[field]
                    return datetime(*time_struct[:6])
                except (ValueError, TypeError):
                    continue
        
        # å¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
        logger.debug("âš ï¸ æ— æ³•è§£æå‘å¸ƒæ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´")
        return datetime.now()
    
    def _extract_description(self, entry: feedparser.util.FeedParserDict) -> str:
        """
        æå–å’Œå¤„ç†æè¿°å†…å®¹
        
        Args:
            entry: feedparseræ¡ç›®
            
        Returns:
            str: å¤„ç†åçš„æè¿°å†…å®¹
        """
        # å°è¯•ä»å¤šä¸ªå­—æ®µè·å–æè¿°
        desc_fields = ['summary', 'description', 'content']
        
        for field in desc_fields:
            if hasattr(entry, field) and entry.get(field):
                description = entry[field]
                
                # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼ï¼ˆå¦‚contentå­—æ®µï¼‰ï¼Œå–ç¬¬ä¸€ä¸ª
                if isinstance(description, list) and description:
                    description = description[0].get('value', '')
                
                # æ¸…æ´—æ–‡æœ¬
                clean_desc = self._clean_text(description)
                
                if clean_desc:
                    # é™åˆ¶é•¿åº¦
                    return clean_desc[:500] + "..." if len(clean_desc) > 500 else clean_desc
        
        return "æ— æè¿°å†…å®¹"
    
    def _generate_content_hash(self, title: str, link: str, description: str) -> str:
        """
        ç”Ÿæˆå†…å®¹å“ˆå¸Œå€¼ç”¨äºå»é‡
        
        Args:
            title: æ ‡é¢˜
            link: é“¾æ¥
            description: æè¿°
            
        Returns:
            str: MD5å“ˆå¸Œå€¼
        """
        content = f"{title}{link}{description}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def _deduplicate_content(self, entries: List[RSSContent]) -> List[RSSContent]:
        """
        ç¬¬4æ­¥ï¼šå†…å®¹å»é‡
        
        Args:
            entries: RSSå†…å®¹åˆ—è¡¨
            
        Returns:
            List[RSSContent]: å»é‡åçš„å†…å®¹åˆ—è¡¨
        """
        seen_hashes = set()
        unique_entries = []
        
        for entry in entries:
            if entry.content_hash not in seen_hashes:
                seen_hashes.add(entry.content_hash)
                unique_entries.append(entry)
            else:
                logger.debug(f"ğŸ”„ é‡å¤å†…å®¹å·²è¿‡æ»¤: {entry.title[:50]}...")
        
        logger.debug(f"âœ… å»é‡å®Œæˆ: {len(entries)}æ¡ â†’ {len(unique_entries)}æ¡")
        return unique_entries
    
    def _process_content_intelligence(self, entries: List[RSSContent]) -> List[RSSContent]:
        """
        ç¬¬5æ­¥ï¼šæ™ºèƒ½å†…å®¹å¤„ç†ï¼ˆæ‘˜è¦ç”Ÿæˆã€æ ‡ç­¾æå–ï¼‰
        
        Args:
            entries: RSSå†…å®¹åˆ—è¡¨
            
        Returns:
            List[RSSContent]: æ™ºèƒ½å¤„ç†åçš„å†…å®¹åˆ—è¡¨
        """
        logger.debug(f"ğŸ§  å¼€å§‹æ™ºèƒ½å†…å®¹å¤„ç†ï¼Œå…±{len(entries)}æ¡")
        
        processed_entries = []
        
        for entry in entries:
            try:
                # ç”Ÿæˆæ™ºèƒ½æ‘˜è¦ï¼ˆç›®å‰ä½¿ç”¨è§„åˆ™ï¼Œæœªæ¥å¯æ¥å…¥LLMï¼‰
                entry.smart_summary = self._generate_summary(entry.title, entry.description)
                
                # æå–æ™ºèƒ½æ ‡ç­¾ï¼ˆç›®å‰ä½¿ç”¨è§„åˆ™ï¼Œæœªæ¥å¯æ¥å…¥LLMï¼‰
                entry.tags = self._extract_tags(entry.title, entry.description)
                
                # è¯†åˆ«å¹³å°ä¿¡æ¯
                entry.platform = self._detect_platform(entry.link)
                
                processed_entries.append(entry)
                
            except Exception as e:
                logger.warning(f"âš ï¸ æ™ºèƒ½å¤„ç†å¤±è´¥: {e}")
                # å³ä½¿æ™ºèƒ½å¤„ç†å¤±è´¥ï¼Œä¹Ÿä¿ç•™åŸå§‹å†…å®¹
                processed_entries.append(entry)
        
        logger.debug(f"âœ… æ™ºèƒ½å¤„ç†å®Œæˆ: {len(processed_entries)}æ¡")
        return processed_entries
    
    def _generate_summary(self, title: str, description: str) -> str:
        """
        ç”Ÿæˆæ™ºèƒ½æ‘˜è¦ï¼ˆç®€åŒ–ç‰ˆè§„åˆ™ï¼Œæœªæ¥å¯æ¥å…¥LLMï¼‰
        
        Args:
            title: æ ‡é¢˜
            description: æè¿°
            
        Returns:
            str: ç”Ÿæˆçš„æ‘˜è¦
        """
        # ç®€åŒ–æ‘˜è¦ç”Ÿæˆé€»è¾‘
        if len(description) <= 100:
            return description
        
        # å–å‰80ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦
        summary = description[:80].rstrip()
        
        # é¿å…åœ¨å¥å­ä¸­é—´æˆªæ–­
        if not summary.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
            last_punct = max(
                summary.rfind('ã€‚'), summary.rfind('ï¼'), 
                summary.rfind('ï¼Ÿ'), summary.rfind('.')
            )
            if last_punct > 30:  # ç¡®ä¿æ‘˜è¦ä¸ä¼šå¤ªçŸ­
                summary = summary[:last_punct + 1]
        
        return summary + "..."
    
    def _extract_tags(self, title: str, description: str) -> List[str]:
        """
        æå–å†…å®¹æ ‡ç­¾ï¼ˆç®€åŒ–ç‰ˆè§„åˆ™ï¼Œæœªæ¥å¯æ¥å…¥LLMï¼‰
        
        Args:
            title: æ ‡é¢˜
            description: æè¿°
            
        Returns:
            List[str]: æå–çš„æ ‡ç­¾åˆ—è¡¨
        """
        content = f"{title} {description}".lower()
        tags = []
        
        # æ¸¸æˆç›¸å…³æ ‡ç­¾
        game_keywords = {
            'æ¸¸æˆ': 'æ¸¸æˆ', 'å¸•é²': 'å¹»å…½å¸•é²', 'è‹±é›„è”ç›Ÿ': 'æ¸¸æˆ',
            'ç‹è€…è£è€€': 'æ¸¸æˆ', 'åŸç¥': 'æ¸¸æˆ', 'steam': 'æ¸¸æˆ'
        }
        
        # å¨±ä¹ç›¸å…³æ ‡ç­¾
        entertainment_keywords = {
            'éŸ³ä¹': 'éŸ³ä¹', 'æ­Œæ‰‹': 'éŸ³ä¹', 'æ¼”å”±ä¼š': 'éŸ³ä¹',
            'ç”µå½±': 'å¨±ä¹', 'ç»¼è‰º': 'å¨±ä¹', 'æ˜æ˜Ÿ': 'å¨±ä¹'
        }
        
        # æŠ€æœ¯ç›¸å…³æ ‡ç­¾
        tech_keywords = {
            'python': 'ç¼–ç¨‹', 'javascript': 'ç¼–ç¨‹', 'ai': 'äººå·¥æ™ºèƒ½',
            'äººå·¥æ™ºèƒ½': 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ': 'æŠ€æœ¯', 'å¼€æº': 'æŠ€æœ¯'
        }
        
        # åˆå¹¶æ‰€æœ‰å…³é”®è¯
        all_keywords = {**game_keywords, **entertainment_keywords, **tech_keywords}
        
        # æ£€æŸ¥å…³é”®è¯åŒ¹é…
        for keyword, tag in all_keywords.items():
            if keyword in content and tag not in tags:
                tags.append(tag)
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°æ ‡ç­¾ï¼Œä½¿ç”¨é»˜è®¤æ ‡ç­¾
        if not tags:
            if 'bilibili' in content or 'bç«™' in content:
                tags.append('è§†é¢‘')
            elif 'weibo' in content or 'å¾®åš' in content:
                tags.append('ç¤¾äº¤')
            else:
                tags.append('å…¶ä»–')
        
        # é™åˆ¶æ ‡ç­¾æ•°é‡
        return tags[:3]
    
    def _detect_platform(self, link: str) -> str:
        """
        ä»é“¾æ¥æ£€æµ‹å¹³å°ä¿¡æ¯
        
        Args:
            link: å†…å®¹é“¾æ¥
            
        Returns:
            str: å¹³å°åç§°
        """
        if not link:
            return "unknown"
        
        domain = urlparse(link).netloc.lower()
        
        platform_mapping = {
            'bilibili.com': 'bilibili',
            'weibo.com': 'weibo', 
            'github.com': 'github',
            'juejin.cn': 'juejin',
            'zhihu.com': 'zhihu',
            'v2ex.com': 'v2ex'
        }
        
        for domain_key, platform in platform_mapping.items():
            if domain_key in domain:
                return platform
        
        return "other"


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•æ–¹æ³•
async def example_usage():
    """RSSå†…å®¹æœåŠ¡ä½¿ç”¨ç¤ºä¾‹"""
    
    # åˆå§‹åŒ–æœåŠ¡
    rss_service = RSSContentService()
    
    # æµ‹è¯•RSS URL
    test_urls = [
        "https://rsshub.app/bilibili/user/video/2267573",  # Bç«™è§†é¢‘
        "https://rsshub.app/weibo/user/1195230310",        # å¾®åšåŠ¨æ€
    ]
    
    for i, rss_url in enumerate(test_urls, 1):
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•RSSæ‹‰å– {i}: {rss_url}")
        print('='*60)
        
        # æ‹‰å–å’Œå¤„ç†RSSå†…å®¹
        contents = rss_service.fetch_rss_content(rss_url, subscription_id=i)
        
        # æ˜¾ç¤ºç»“æœ
        for content in contents:
            print(f"ğŸ“„ æ ‡é¢˜: {content.title}")
            print(f"ğŸ”— é“¾æ¥: {content.link}")
            print(f"ğŸ“ æ‘˜è¦: {content.smart_summary}")
            print(f"ğŸ·ï¸  æ ‡ç­¾: {content.tags}")
            print(f"ğŸ“± å¹³å°: {content.platform}")
            print(f"ğŸ“… å‘å¸ƒ: {content.pub_date}")
            print("-" * 50)


if __name__ == "__main__":
    import asyncio
    asyncio.run(example_usage()) 