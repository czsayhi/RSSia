#!/usr/bin/env python3
"""
RSSå†…å®¹æ‹‰å–å’Œå¤„ç†æœåŠ¡
è´Ÿè´£RSSå†…å®¹çš„æ‹‰å–ã€è§£æã€å¤„ç†ã€å­˜å‚¨ç­‰æ ¸å¿ƒåŠŸèƒ½
v3.0: ç®€åŒ–æ¶æ„ï¼Œä½¿ç”¨è‡ªå»ºRSShubå®ä¾‹ï¼Œç§»é™¤å¤æ‚é‡è¯•é€»è¾‘
v3.1: å¢åŠ å†…å®¹æ—¶é—´èŒƒå›´æ§åˆ¶ï¼Œåªè·å–æŒ‡å®šå¤©æ•°å†…çš„å†…å®¹
"""

import re
import time
import random
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any, Tuple
from urllib.parse import urlparse

import feedparser
import requests
from bs4 import BeautifulSoup
from loguru import logger

from .shared_content_service import SharedContentService


class RSSContentService:
    """RSSå†…å®¹å¤„ç†æœåŠ¡ï¼ˆv3.1 - å¢åŠ æ—¶é—´æ§åˆ¶ï¼‰"""
    
    def __init__(
        self, 
        timeout: int = 15, 
        rsshub_base_url: str = None,
        content_time_range_days: int = 30,
        test_mode: bool = False,
        test_limit: int = 1
    ):
        """
        åˆå§‹åŒ–RSSå†…å®¹æœåŠ¡
        
        Args:
            timeout: HTTPè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’) 
            rsshub_base_url: è‡ªå»ºRSShubå®ä¾‹åœ°å€
            content_time_range_days: å†…å®¹æ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼‰ï¼Œåªè·å–æ­¤èŒƒå›´å†…çš„å†…å®¹
            test_mode: æµ‹è¯•æ¨¡å¼ï¼Œå¯ç”¨åå°†é™åˆ¶æ‹‰å–å†…å®¹æ•°é‡
            test_limit: æµ‹è¯•æ¨¡å¼ä¸‹çš„æœ€å¤§å†…å®¹æ•°é‡
        """
        self.timeout = timeout
        
        # ç®€åŒ–çš„User-Agent
        self.user_agent = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        
        # è‡ªå»ºRSShubå®ä¾‹é…ç½®
        self.rsshub_base_url = rsshub_base_url or "http://rssia-hub:1200"
        
        # å†…å®¹æ—¶é—´èŒƒå›´æ§åˆ¶
        self.content_time_range_days = content_time_range_days
        self.time_cutoff = datetime.now() - timedelta(days=content_time_range_days)
        
        # æµ‹è¯•æ¨¡å¼é…ç½®
        self.test_mode = test_mode
        self.test_limit = test_limit
        
        # ç®€åŒ–çš„é‡è¯•é…ç½®
        self.retry_config = {
            'max_retries': 2,          # å‡å°‘åˆ°2æ¬¡é‡è¯•
            'base_delay': 1,           # 1ç§’åŸºç¡€å»¶è¿Ÿ
        }
        
        self.shared_content_service = SharedContentService()
        logger.info(
            f"ğŸ”§ RSSå†…å®¹æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆv3.1 - æ—¶é—´æ§åˆ¶ç‰ˆï¼‰- "
            f"RSShub: {self.rsshub_base_url}, "
            f"æ—¶é—´èŒƒå›´: {content_time_range_days}å¤©, "
            f"æµ‹è¯•æ¨¡å¼: {'å¼€å¯(é™åˆ¶'+str(test_limit)+'æ¡)' if test_mode else 'å…³é—­'}"
        )
    
    async def fetch_and_store_rss_content(
        self, 
        rss_url: str, 
        subscription_id: int, 
        user_id: int
    ) -> Dict[str, Any]:
        """
        æ‹‰å–å’Œå­˜å‚¨RSSå†…å®¹çš„ä¸»å…¥å£æ–¹æ³•ï¼ˆç®€åŒ–ç‰ˆï¼‰
        
        Args:
            rss_url: RSSè®¢é˜…URL
            subscription_id: è®¢é˜…ID
            user_id: ç”¨æˆ·ID
            
        Returns:
            Dict: å¤„ç†ç»“æœç»Ÿè®¡
        """
        logger.info(f"ğŸš€ å¼€å§‹æ‹‰å–RSSå†…å®¹: {rss_url}, user_id={user_id}")
        
        try:
            # ç¬¬1æ­¥ï¼šå‘é€HTTPè¯·æ±‚æ‹‰å–RSSåŸå§‹æ•°æ®
            raw_content = self._fetch_raw_rss(rss_url)
            if not raw_content:
                return {'error': 'HTTPè¯·æ±‚å¤±è´¥'}
            
            # ç¬¬2æ­¥ï¼šä½¿ç”¨feedparserè§£æRSS/Atomå†…å®¹
            feed_data = self._parse_rss_feed(raw_content)
            if not feed_data:
                return {'error': 'RSSè§£æå¤±è´¥'}
            
            # ç¬¬3æ­¥ï¼šæå–å¹¶æ ‡å‡†åŒ–å†…å®¹æ•°æ®
            rss_items = self._extract_and_standardize_entries(feed_data)
            
            # ç¬¬4æ­¥ï¼šä½¿ç”¨æ–°æ¶æ„å­˜å‚¨å†…å®¹
            result = await self.shared_content_service.store_rss_content(
                rss_items=rss_items,
                subscription_id=subscription_id,
                user_id=user_id
            )
            
            # ğŸ”¥ ç¬¬5æ­¥ï¼šAIé¢„å¤„ç† - åŸºäºAIå­—æ®µæ˜¯å¦ä¸ºç©º
            need_ai_processing_ids = result.get('need_ai_processing_ids', [])
            if need_ai_processing_ids:
                ai_result = await self._trigger_ai_processing(need_ai_processing_ids, user_id, subscription_id)
                result['ai_processing'] = ai_result
            
            logger.success(
                f"âœ… RSSå†…å®¹å¤„ç†å®Œæˆ: {rss_url} | "
                f"å¤„ç†{result.get('total_processed', 0)}æ¡ï¼Œ"
                f"æ–°å¢{result.get('new_content', 0)}æ¡ï¼Œ"
                f"å¤ç”¨{result.get('reused_content', 0)}æ¡ï¼Œ"
                f"AIå¤„ç†{result.get('ai_processing', {}).get('processed', 0)}æ¡"
            )
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ RSSå†…å®¹æ‹‰å–å¤±è´¥: {rss_url} | é”™è¯¯: {e}")
            return {'error': str(e)}
    
    async def _trigger_ai_processing(
        self, 
        need_ai_processing_ids: List[int], 
        user_id: int,
        subscription_id: int
    ) -> Dict[str, Any]:
        """
        ç¬¬5æ­¥ï¼šè§¦å‘AIé¢„å¤„ç†ï¼ˆåŸºäºAIå­—æ®µæ˜¯å¦ä¸ºç©ºï¼‰
        
        Args:
            need_ai_processing_ids: éœ€è¦AIå¤„ç†çš„å†…å®¹IDåˆ—è¡¨ï¼ˆæ–°å†…å®¹+ç¼ºå°‘AIç»“æœçš„æ—§å†…å®¹ï¼‰
            user_id: ç”¨æˆ·ID
            subscription_id: è®¢é˜…ID
            
        Returns:
            Dict: AIå¤„ç†ç»“æœç»Ÿè®¡
        """
        try:
            logger.info(f"ğŸ§  å¼€å§‹AIé¢„å¤„ç†: {len(need_ai_processing_ids)}æ¡éœ€è¦å¤„ç†çš„å†…å®¹, user_id={user_id}")
            
            # ä»æ•°æ®åº“è¯»å–éœ€è¦AIå¤„ç†çš„å†…å®¹
            db_contents = await self.shared_content_service.get_contents_by_ids(need_ai_processing_ids)
            if not db_contents:
                logger.warning("âš ï¸ æ— æ³•ä»æ•°æ®åº“è¯»å–éœ€è¦å¤„ç†çš„å†…å®¹")
                return {'processed': 0, 'success': 0, 'failed': 0}
            
            # å¯¼å…¥AIå†…å®¹å¤„ç†å™¨
            from .ai_content_processor import ai_content_processor
            from ..models.content import RSSContent
            
            # ä»æ•°æ®åº“è®°å½•åˆ›å»ºRSSContentå¯¹è±¡ï¼ˆåŒ…å«content_idä¿¡æ¯ï¼‰
            rss_content_objects = []
            for db_content in db_contents:
                try:
                    # åŸºäºæ•°æ®åº“è®°å½•åˆ›å»ºRSSContentå¯¹è±¡ - åŒ…å«æ‰€æœ‰æ ‡å‡†å­—æ®µ
                    rss_content = RSSContent(
                        content_id=db_content['content_id'],  # ğŸ”¥ å…³é”®ï¼šåŒ…å«content_id
                        subscription_id=subscription_id,
                        content_hash=db_content['content_hash'],
                        title=db_content['title'],
                        original_link=db_content['original_link'],
                        published_at=db_content['published_at'],
                        description=db_content['description'],
                        description_text=db_content['description_text'],
                        author=db_content['author'],
                        platform=db_content['platform'],
                        feed_title=db_content['feed_title'],
                        cover_image=db_content['cover_image'],
                        content_type=db_content['content_type']
                    )
                    rss_content_objects.append(rss_content)
                except Exception as e:
                    logger.warning(f"âš ï¸ æ•°æ®åº“å†…å®¹è½¬æ¢å¤±è´¥ï¼Œè·³è¿‡: {db_content.get('title', 'Unknown')[:30]}... | é”™è¯¯: {e}")
                    continue
            
            if not rss_content_objects:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„å†…å®¹å¯ä¾›AIå¤„ç†")
                return {'processed': 0, 'success': 0, 'failed': 0}
            
            # è°ƒç”¨AIå†…å®¹å¤„ç†å™¨
            processed_entries = await ai_content_processor.process_content_intelligence(rss_content_objects)
            
            # ç»Ÿè®¡å¤„ç†ç»“æœ
            result = {
                'processed': len(rss_content_objects),
                'success': len(processed_entries),
                'failed': len(rss_content_objects) - len(processed_entries),
                'success_rate': round(len(processed_entries) / len(rss_content_objects) * 100, 1) if rss_content_objects else 0
            }
            
            logger.success(f"âœ… AIé¢„å¤„ç†å®Œæˆ: {result}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ AIé¢„å¤„ç†å¤±è´¥: {e}")
            # AIå¤„ç†å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œè¿”å›å¤±è´¥ç»Ÿè®¡
            return {
                'processed': len(need_ai_processing_ids),
                'success': 0,
                'failed': len(need_ai_processing_ids),
                'error': str(e)
            }
    
    def _fetch_raw_rss(self, rss_url: str) -> Optional[bytes]:
        """
        ç¬¬1æ­¥ï¼šæ‹‰å–RSSåŸå§‹æ•°æ®ï¼ˆv3.0 - ç®€åŒ–ç‰ˆæœ¬ï¼‰
        ç§»é™¤å¤šå®ä¾‹è½®æ¢ï¼Œä½¿ç”¨è‡ªå»ºRSShubå®ä¾‹
        
        Args:
            rss_url: RSS URL
            
        Returns:
            Optional[bytes]: RSSåŸå§‹å†…å®¹å­—èŠ‚æ•°æ®
        """
        logger.debug(f"ğŸ“¡ å¼€å§‹æ‹‰å–RSS: {rss_url}")
        
        # æ„å»ºå®Œæ•´URL
        if rss_url.startswith('http'):
            final_url = rss_url
        else:
            final_url = f"{self.rsshub_base_url}{rss_url}"
        
        # ç®€åŒ–çš„é‡è¯•é€»è¾‘
        for attempt in range(self.retry_config['max_retries'] + 1):
            try:
                # ç®€åŒ–çš„è¯·æ±‚å¤´
                headers = {
                    'User-Agent': self.user_agent,
                    'Accept': 'application/rss+xml, application/xml, text/xml, */*',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
                }
                
                if attempt > 0:
                    time.sleep(self.retry_config['base_delay'] * attempt)
                
                logger.debug(f"ğŸ”„ å°è¯• {attempt + 1}/{self.retry_config['max_retries'] + 1}: {final_url}")
                
                response = requests.get(
                    final_url,
                    headers=headers,
                    timeout=self.timeout,
                    allow_redirects=True
                )
                
                response.raise_for_status()
                
                if response.content:
                    logger.success(f"âœ… æˆåŠŸè·å–RSSå†…å®¹ï¼Œå¤§å°: {len(response.content)} bytes")
                    return response.content
                else:
                    logger.warning("âš ï¸ å“åº”å†…å®¹ä¸ºç©º")
                    
            except requests.exceptions.RequestException as e:
                logger.warning(f"âš ï¸ è¯·æ±‚å¤±è´¥ (å°è¯•{attempt + 1}): {e}")
                if attempt == self.retry_config['max_retries']:
                    logger.error(f"âŒ æ‰€æœ‰é‡è¯•å°è¯•å¤±è´¥: {final_url}")
                    
        return None
    
    def _parse_rss_feed(self, raw_content: bytes) -> Optional[feedparser.FeedParserDict]:
        """
        ç¬¬2æ­¥ï¼šä½¿ç”¨feedparserè§£æRSS/Atomå†…å®¹
        
        Args:
            raw_content: RSSåŸå§‹å­—èŠ‚å†…å®¹
            
        Returns:
            Optional[FeedParserDict]: feedparserè§£æç»“æœ
        """
        logger.debug("ğŸ” å¼€å§‹è§£æRSSå†…å®¹...")
        
        try:
            # ä½¿ç”¨feedparserè§£æRSS/Atomæ ¼å¼
            feed = feedparser.parse(raw_content)
            
            # æ£€æŸ¥è§£ææ˜¯å¦æˆåŠŸ
            if hasattr(feed, 'bozo') and feed.bozo:
                logger.warning(f"âš ï¸ RSSæ ¼å¼å¯èƒ½æœ‰é—®é¢˜: {feed.bozo_exception}")
            
            # éªŒè¯feedæ˜¯å¦åŒ…å«entries
            if not hasattr(feed, 'entries') or not feed.entries:
                logger.warning("âš ï¸ RSSè§£æç»“æœä¸­æ²¡æœ‰æ¡ç›®æ•°æ®")
                return None
            
            logger.debug(f"âœ… RSSè§£ææˆåŠŸ: æ ‡é¢˜={feed.feed.get('title', 'æœªçŸ¥')}, æ¡ç›®æ•°={len(feed.entries)}")
            return feed
            
        except Exception as e:
            logger.error(f"âŒ RSSè§£æå¼‚å¸¸: {e}")
            return None
    
    def _extract_and_standardize_entries(self, feed: feedparser.FeedParserDict) -> List[Dict[str, Any]]:
        """
        ç¬¬3æ­¥ï¼šæå–å¹¶æ ‡å‡†åŒ–RSSæ¡ç›®æ•°æ®ï¼ˆv3.1 - å¢åŠ æ—¶é—´è¿‡æ»¤ï¼‰
        
        Args:
            feed: feedparserè§£æç»“æœ
            
        Returns:
            List[Dict]: æ ‡å‡†åŒ–çš„RSSå†…å®¹åˆ—è¡¨ï¼ˆåªåŒ…å«æ—¶é—´èŒƒå›´å†…çš„å†…å®¹ï¼‰
        """
        logger.debug(f"ğŸ“ å¼€å§‹æå–RSSæ¡ç›®æ•°æ®ï¼ˆæ—¶é—´èŒƒå›´: {self.content_time_range_days}å¤©ï¼‰ï¼Œå…±{len(feed.entries)}æ¡")
        
        rss_items = []
        filtered_count = 0
        
        # æå–Feedçº§åˆ«ä¿¡æ¯
        feed_info = {
            'feed_title': self._clean_text(feed.feed.get('title', 'æœªçŸ¥è®¢é˜…æº')),
            'feed_description': self._clean_text(feed.feed.get('description', '')),
            'feed_link': feed.feed.get('link', ''),
            'feed_image_url': self._extract_feed_image(feed),
            'feed_last_build_date': self._parse_feed_date(feed),
            'platform': self._detect_platform_from_feed(feed)
        }
        
        for entry in feed.entries:
            try:
                # å¤„ç†å‘å¸ƒæ—¶é—´
                published_at = self._parse_publish_date(entry)
                
                # ğŸ”¥ æ—¶é—´èŒƒå›´è¿‡æ»¤ï¼šåªä¿ç•™æŒ‡å®šå¤©æ•°å†…çš„å†…å®¹
                if published_at < self.time_cutoff:
                    filtered_count += 1
                    logger.debug(f"â° è¿‡æ»¤æ—§å†…å®¹: {entry.get('title', '')[:30]}... (å‘å¸ƒæ—¶é—´: {published_at.strftime('%Y-%m-%d')})")
                    continue
                
                # æå–åŸºç¡€å­—æ®µ
                title = self._clean_text(entry.get('title', 'æ— æ ‡é¢˜'))
                original_link = entry.get('link', '')
                
                # æå–å’Œæ¸…æ´—æè¿°å†…å®¹
                description = self._extract_description(entry)
                description_text = self._clean_text(description)
                
                # ä½œè€…ä¿¡æ¯ï¼ˆå¸¦å…œåº•é€»è¾‘ï¼‰
                author = self._extract_author_with_fallback(entry, feed_info['feed_title'])
                
                # å†…å®¹ç±»å‹åˆ¤æ–­
                content_type = self._determine_content_type(entry, description)
                
                # æå–åª’ä½“é¡¹
                media_items = self._extract_media_items(entry, description)
                cover_image = self._determine_cover_image(media_items)
                
                # åˆ›å»ºæ ‡å‡†åŒ–å†…å®¹é¡¹
                rss_item = {
                    'title': title,
                    'description': description,
                    'description_text': description_text,
                    'author': author,
                    'published_at': published_at,
                    'original_link': original_link,
                    'content_type': content_type,
                    'platform': feed_info['platform'],
                    'guid': entry.get('guid', ''),
                    'cover_image': cover_image,
                    'media_items': media_items,
                    
                    # Feedçº§åˆ«ä¿¡æ¯
                    'feed_title': feed_info['feed_title'],
                    'feed_description': feed_info['feed_description'],
                    'feed_link': feed_info['feed_link'],
                    'feed_image_url': feed_info['feed_image_url'],
                    'feed_last_build_date': feed_info['feed_last_build_date']
                }
                
                rss_items.append(rss_item)
                logger.debug(f"ğŸ“„ æå–æ¡ç›®: {title[:50]}... (å‘å¸ƒæ—¶é—´: {published_at.strftime('%Y-%m-%d %H:%M')})")
                
                # ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶å†…å®¹æ•°é‡
                if self.test_mode and len(rss_items) >= self.test_limit:
                    logger.info(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šå·²è¾¾åˆ°é™åˆ¶æ•°é‡({self.test_limit}æ¡)ï¼Œåœæ­¢æå–")
                    break
                
            except Exception as e:
                logger.warning(f"âš ï¸ æ¡ç›®æå–å¤±è´¥: {e}")
                continue
        
        logger.success(
            f"âœ… æ¡ç›®æå–å®Œæˆ{'ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰' if self.test_mode else 'ï¼ˆæ—¶é—´æ§åˆ¶ç‰ˆï¼‰'}: "
            f"ä¿ç•™{len(rss_items)}æ¡ï¼Œè¿‡æ»¤{filtered_count}æ¡æ—§å†…å®¹"
        )
        return rss_items
    
    def _extract_author_with_fallback(self, entry: feedparser.util.FeedParserDict, feed_title: str) -> Optional[str]:
        """
        ä½œè€…ä¿¡æ¯æå–ï¼ˆå¸¦å…œåº•é€»è¾‘ï¼‰
        
        Args:
            entry: feedparseræ¡ç›®
            feed_title: è®¢é˜…æºæ ‡é¢˜
            
        Returns:
            Optional[str]: ä½œè€…ä¿¡æ¯
        """
        # 1. å°è¯•ä»æ¡ç›®ä¸­æå–ä½œè€…
        author = entry.get('author', '').strip()
        if author:
            return author
        
        # 2. ä½¿ç”¨è®¢é˜…æºæ ‡é¢˜å…œåº•ï¼ˆæ¸…ç†å¹³å°ç‰¹å®šåç¼€ï¼‰
        if feed_title:
            # æ¸…ç†ä¸åŒå¹³å°çš„æ ‡é¢˜åç¼€
            clean_title = feed_title
            suffixes_to_remove = [
                'çš„å¾®åš', ' çš„ bilibili ç©ºé—´', 'çš„bilibiliç©ºé—´',
                ' - çŸ¥ä¹', 'çš„çŸ¥ä¹ä¸“æ ', ' | å°‘æ•°æ´¾'
            ]
            
            for suffix in suffixes_to_remove:
                clean_title = clean_title.replace(suffix, '')
            
            return clean_title.strip() if clean_title.strip() else None
        
        return None
    
    def _determine_content_type(self, entry: feedparser.util.FeedParserDict, description: str) -> str:
        """
        åˆ¤æ–­å†…å®¹ç±»å‹
        
        Args:
            entry: feedparseræ¡ç›®
            description: æè¿°å†…å®¹
            
        Returns:
            str: å†…å®¹ç±»å‹
        """
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è§†é¢‘
        if any(keyword in description.lower() for keyword in ['video', 'è§†é¢‘', 'bilibili.com/video']):
            return 'video'
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
        if any(keyword in description.lower() for keyword in ['<img', 'image', 'å›¾ç‰‡']):
            return 'image_text'
        
        return 'text'
    
    def _extract_media_items(self, entry: feedparser.util.FeedParserDict, description: str) -> List[Dict[str, Any]]:
        """
        æå–åª’ä½“é¡¹
        
        Args:
            entry: feedparseræ¡ç›®
            description: æè¿°å†…å®¹
            
        Returns:
            List[Dict]: åª’ä½“é¡¹åˆ—è¡¨
        """
        media_items = []
        
        try:
            # ä»æè¿°ä¸­æå–å›¾ç‰‡
            soup = BeautifulSoup(description, 'html.parser')
            images = soup.find_all('img')
            
            for img in images:
                src = img.get('src', '')
                if src:
                    media_items.append({
                        'url': src,
                        'type': 'image',
                        'description': img.get('alt', ''),
                        'duration': None
                    })
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è§†é¢‘é“¾æ¥
            if 'bilibili.com/video' in description:
                # ç®€å•çš„è§†é¢‘æ£€æµ‹
                media_items.append({
                    'url': entry.get('link', ''),
                    'type': 'video',
                    'description': 'è§†é¢‘å†…å®¹',
                    'duration': None  # é¢„ç•™å­—æ®µ
                })
        
        except Exception as e:
            logger.debug(f"åª’ä½“é¡¹æå–å¤±è´¥: {e}")
        
        return media_items
    
    def _determine_cover_image(self, media_items: List[Dict[str, Any]]) -> Optional[str]:
        """
        ç¡®å®šå°é¢å›¾ç‰‡
        
        Args:
            media_items: åª’ä½“é¡¹åˆ—è¡¨
            
        Returns:
            Optional[str]: å°é¢å›¾ç‰‡URL
        """
        for item in media_items:
            if item.get('type') == 'image' and item.get('url'):
                return item['url']
        return None
    
    def _extract_feed_image(self, feed: feedparser.FeedParserDict) -> Optional[str]:
        """æå–Feedå›¾åƒ"""
        if hasattr(feed.feed, 'image') and feed.feed.image:
            return feed.feed.image.get('href', '')
        return None
    
    def _parse_feed_date(self, feed: feedparser.FeedParserDict) -> Optional[datetime]:
        """è§£æFeedæ„å»ºæ—¶é—´"""
        if hasattr(feed.feed, 'updated_parsed') and feed.feed.updated_parsed:
            try:
                return datetime(*feed.feed.updated_parsed[:6])
            except (ValueError, TypeError):
                pass
        return None
    
    def _detect_platform_from_feed(self, feed: feedparser.FeedParserDict) -> str:
        """ä»Feedä¿¡æ¯æ£€æµ‹å¹³å°"""
        feed_link = feed.feed.get('link', '')
        feed_title = feed.feed.get('title', '')
        
        if 'bilibili' in feed_link.lower() or 'bilibili' in feed_title.lower():
            return 'bilibili'
        elif 'weibo' in feed_link.lower() or 'å¾®åš' in feed_title:
            return 'weibo'
        elif 'github' in feed_link.lower():
            return 'github'
        elif 'zhihu' in feed_link.lower() or 'çŸ¥ä¹' in feed_title:
            return 'zhihu'
        
        return 'other'
    
    def _clean_text(self, text: str) -> str:
        """
        æ¸…æ´—æ–‡æœ¬å†…å®¹ï¼šå»é™¤HTMLæ ‡ç­¾ã€å¤šä½™ç©ºç™½å­—ç¬¦ç­‰
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            str: æ¸…æ´—åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # ä½¿ç”¨BeautifulSoupå»é™¤HTMLæ ‡ç­¾
        clean_text = BeautifulSoup(text, 'html.parser').get_text()
        
        # å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        
        return clean_text
    
    def _parse_publish_date(self, entry: feedparser.util.FeedParserDict) -> datetime:
        """
        è§£æå‘å¸ƒæ—¶é—´
        
        Args:
            entry: feedparseræ¡ç›®
            
        Returns:
            datetime: è§£æåçš„æ—¶é—´å¯¹è±¡
        """
        # å°è¯•ä»å¤šä¸ªå­—æ®µè·å–æ—¶é—´
        time_fields = ['published_parsed', 'updated_parsed', 'created_parsed']
        
        for field in time_fields:
            if hasattr(entry, field) and entry.get(field):
                try:
                    time_struct = entry[field]
                    return datetime(*time_struct[:6])
                except (ValueError, TypeError):
                    continue
        
        # å¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
        logger.debug("âš ï¸ æ— æ³•è§£æå‘å¸ƒæ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´")
        return datetime.now()
    
    def _extract_description(self, entry: feedparser.util.FeedParserDict) -> str:
        """
        æå–å’Œå¤„ç†æè¿°å†…å®¹
        
        Args:
            entry: feedparseræ¡ç›®
            
        Returns:
            str: å¤„ç†åçš„æè¿°å†…å®¹
        """
        # å°è¯•ä»å¤šä¸ªå­—æ®µè·å–æè¿°
        desc_fields = ['summary', 'description', 'content']
        
        for field in desc_fields:
            if hasattr(entry, field) and entry.get(field):
                description = entry[field]
                
                # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼ï¼ˆå¦‚contentå­—æ®µï¼‰ï¼Œå–ç¬¬ä¸€ä¸ª
                if isinstance(description, list) and description:
                    description = description[0].get('value', '')
                
                if description:
                    return description
        
        return "æ— æè¿°å†…å®¹"
    

