#!/usr/bin/env python3
"""
AIæœåŠ¡ç®¡ç†å™¨
ç»Ÿä¸€ç®¡ç†LLMæœåŠ¡ã€å‘é‡æœåŠ¡å’Œpromptæ¨¡ç‰ˆ
æä¾›æ ‡å‡†åŒ–çš„AIè°ƒç”¨æ¥å£ï¼Œé¿å…æœåŠ¡é—´ç›´æ¥ä¾èµ–
"""
import json
from pathlib import Path
from typing import Optional, Dict, Any, List
from loguru import logger
import asyncio


class AIServiceManager:
    """AIæœåŠ¡ç»Ÿä¸€ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = "data/rss_subscriber.db"):
        """
        åˆå§‹åŒ–AIæœåŠ¡ç®¡ç†å™¨
        
        Args:
            db_path: æ•°æ®åº“è·¯å¾„ï¼ˆå‘é‡æœåŠ¡å¯èƒ½éœ€è¦ï¼‰
        """
        self.db_path = db_path
        
        # AIæœåŠ¡å®ä¾‹
        self.llm_service = None
        self.vector_service = None
        
        # Promptæ¨¡ç‰ˆç¼“å­˜
        self.prompt_templates = {}
        
        # åˆå§‹åŒ–æœåŠ¡
        self._init_services()
        self._load_prompt_templates()
    
    def _init_services(self):
        """åˆå§‹åŒ–AIæœåŠ¡"""
        # åˆå§‹åŒ–LLMæœåŠ¡
        try:
            self.llm_service = self._init_llm_service()
            if self.llm_service:
                logger.info("ğŸ¤– LLMæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("âš ï¸ LLMæœåŠ¡ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨è§„åˆ™å…œåº•")
        except Exception as e:
            logger.warning(f"âš ï¸ LLMæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            self.llm_service = None
        
        # åˆå§‹åŒ–å‘é‡æœåŠ¡
        try:
            self.vector_service = self._init_vector_service()
            if self.vector_service:
                logger.info("ğŸ”® å‘é‡æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("âš ï¸ å‘é‡æœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡å‘é‡åŒ–å¤„ç†")
        except Exception as e:
            logger.warning(f"âš ï¸ å‘é‡æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            self.vector_service = None
    
    def _init_llm_service(self):
        """åˆå§‹åŒ–LLMæœåŠ¡"""
        try:
            from scripts.qwen3_chat import Qwen3Chat
            llm = Qwen3Chat()
            return llm
        except ImportError as e:
            logger.warning(f"ğŸ¤– Qwen3Chatæ¨¡å—ä¸å¯ç”¨: {e}")
            return None
        except Exception as e:
            logger.warning(f"ğŸ¤– LLMåˆå§‹åŒ–å¤±è´¥: {e}")
            return None
    
    def _init_vector_service(self):
        """åˆå§‹åŒ–å‘é‡æœç´¢æœåŠ¡"""
        try:
            from scripts.vector_search import VectorSearchService
            vector_service = VectorSearchService()
            return vector_service
        except ImportError as e:
            logger.warning(f"ğŸ”® VectorSearchServiceæ¨¡å—ä¸å¯ç”¨: {e}")
            return None
        except Exception as e:
            logger.warning(f"ğŸ”® å‘é‡æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            return None
    
    def _load_prompt_templates(self):
        """åŠ è½½promptæ¨¡ç‰ˆ"""
        try:
            # å½“å‰ä¸»è¦çš„promptæ¨¡ç‰ˆæ–‡ä»¶
            template_path = Path(__file__).parent.parent.parent / "optimized_prompt_template.txt"
            
            if template_path.exists():
                with open(template_path, 'r', encoding='utf-8') as f:
                    self.prompt_templates["content_analysis"] = f.read()
                logger.info("ğŸ“ Promptæ¨¡ç‰ˆåŠ è½½æˆåŠŸ")
            else:
                # ä½¿ç”¨å…œåº•æ¨¡ç‰ˆ
                self.prompt_templates["content_analysis"] = self._get_fallback_prompt_template()
                logger.warning("âš ï¸ ä¸»promptæ¨¡ç‰ˆæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨å…œåº•æ¨¡ç‰ˆ")
                
        except Exception as e:
            logger.error(f"âŒ åŠ è½½promptæ¨¡ç‰ˆå¤±è´¥: {e}")
            self.prompt_templates["content_analysis"] = self._get_fallback_prompt_template()
    
    def _get_fallback_prompt_template(self) -> str:
        """è·å–å…œåº•çš„promptæ¨¡ç‰ˆ"""
        return """ä½ æ˜¯ä¸€åå†…å®¹åˆ†æä¸“å®¶ã€‚åŸºäºä»¥ä¸‹å†…å®¹ç”Ÿæˆä¸»é¢˜ã€æ ‡ç­¾å’Œæ‘˜è¦ï¼š

è¾“å…¥å†…å®¹ï¼š
{title}
{description}
{description_text}
{author}
{platform}
{feed_title}

è¯·æŒ‰JSONæ ¼å¼è¾“å‡ºï¼š
{
  "topics": "ä¸»é¢˜",
  "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"],
  "summary": "æ‘˜è¦"
}"""
    
    # =================
    # å…¬å¼€è°ƒç”¨æ¥å£
    # =================
    
    async def call_llm(self, prompt: str) -> Optional[str]:
        """
        è°ƒç”¨LLMæœåŠ¡
        
        Args:
            prompt: è¾“å…¥prompt
            
        Returns:
            Optional[str]: LLMå“åº”ç»“æœï¼Œå¤±è´¥è¿”å›None
        """
        if not self.llm_service:
            logger.warning("ğŸ¤– LLMæœåŠ¡ä¸å¯ç”¨")
            return None
        
        try:
            # è°ƒç”¨LLMç”Ÿæˆç»“æœ
            response = await self._execute_llm_call(prompt)
            return response
        except Exception as e:
            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            return None
    
    async def _execute_llm_call(self, prompt: str) -> Optional[str]:
        """æ‰§è¡ŒLLMè°ƒç”¨ï¼ˆå¯èƒ½éœ€è¦æ ¹æ®å…·ä½“LLMæœåŠ¡è°ƒæ•´ï¼‰"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„LLMæœåŠ¡æ¥å£è¿›è¡Œè°ƒç”¨
        # ç›®å‰åŸºäºqwen3_chatçš„æ¥å£è®¾è®¡
        if hasattr(self.llm_service, 'generate_response'):
            # generate_responseæ˜¯åŒæ­¥æ–¹æ³•ï¼Œä¸éœ€è¦await
            return self.llm_service.generate_response(prompt)
        elif hasattr(self.llm_service, 'chat'):
            # chatæ–¹æ³•å¦‚æœæ˜¯åŒæ­¥çš„ï¼Œä¸éœ€è¦await
            if asyncio.iscoroutinefunction(self.llm_service.chat):
                return await self.llm_service.chat(prompt)
            else:
                return self.llm_service.chat(prompt)
        else:
            logger.error("âŒ LLMæœåŠ¡æ¥å£ä¸åŒ¹é…")
            return None
    
    async def call_vector_service(self, text: str) -> Optional[List[float]]:
        """
        è°ƒç”¨å‘é‡æœåŠ¡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            Optional[List[float]]: å‘é‡ç»“æœï¼Œå¤±è´¥è¿”å›None
        """
        if not self.vector_service:
            logger.warning("ğŸ”® å‘é‡æœåŠ¡ä¸å¯ç”¨")
            return None
        
        try:
            # è°ƒç”¨å‘é‡æœåŠ¡ç”Ÿæˆå‘é‡
            vector = await self._execute_vector_call(text)
            return vector
        except Exception as e:
            logger.error(f"âŒ å‘é‡æœåŠ¡è°ƒç”¨å¤±è´¥: {e}")
            return None
    
    async def _execute_vector_call(self, text: str) -> Optional[List[float]]:
        """æ‰§è¡Œå‘é‡åŒ–è°ƒç”¨"""
        if hasattr(self.vector_service, 'get_embedding'):
            # æ£€æŸ¥æ˜¯å¦ä¸ºasyncæ–¹æ³•
            if asyncio.iscoroutinefunction(self.vector_service.get_embedding):
                return await self.vector_service.get_embedding(text)
            else:
                return self.vector_service.get_embedding(text)
        elif hasattr(self.vector_service, 'encode_text'):
            # encode_textæ˜¯åŒæ­¥æ–¹æ³•
            return self.vector_service.encode_text(text)
        elif hasattr(self.vector_service, 'encode'):
            # encodeæ–¹æ³•å¦‚æœæ˜¯åŒæ­¥çš„ï¼Œä¸éœ€è¦await
            if asyncio.iscoroutinefunction(self.vector_service.encode):
                return await self.vector_service.encode(text)
            else:
                return self.vector_service.encode(text)
        else:
            logger.error("âŒ å‘é‡æœåŠ¡æ¥å£ä¸åŒ¹é…")
            return None
    
    async def store_vector(self, content_id: int, vector: List[float], metadata: Dict[str, Any] = None):
        """
        å­˜å‚¨å‘é‡æ•°æ®
        
        Args:
            content_id: å†…å®¹ID
            vector: å‘é‡æ•°æ®
            metadata: å…ƒæ•°æ®
        """
        if not self.vector_service:
            logger.warning("ğŸ”® å‘é‡æœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡å­˜å‚¨")
            return
        
        try:
            if hasattr(self.vector_service, 'add_vector'):
                await self.vector_service.add_vector(
                    id=str(content_id),
                    vector=vector,
                    metadata=metadata or {}
                )
                logger.debug(f"ğŸ”® å‘é‡å­˜å‚¨æˆåŠŸ: content_id={content_id}")
            else:
                logger.warning("âš ï¸ å‘é‡æœåŠ¡ä¸æ”¯æŒå­˜å‚¨åŠŸèƒ½")
        except Exception as e:
            logger.error(f"âŒ å‘é‡å­˜å‚¨å¤±è´¥: {e}")
    
    def get_prompt_template(self, template_name: str) -> str:
        """
        è·å–promptæ¨¡ç‰ˆ
        
        Args:
            template_name: æ¨¡ç‰ˆåç§°
            
        Returns:
            str: promptæ¨¡ç‰ˆå†…å®¹
        """
        return self.prompt_templates.get(template_name, self._get_fallback_prompt_template())
    
    def prepare_prompt(self, template_name: str, **kwargs) -> str:
        """
        å‡†å¤‡promptè¾“å…¥
        
        Args:
            template_name: æ¨¡ç‰ˆåç§°
            **kwargs: æ¨¡ç‰ˆå‚æ•°
            
        Returns:
            str: æ ¼å¼åŒ–åçš„prompt
        """
        template = self.get_prompt_template(template_name)
        try:
            return template.format(**kwargs)
        except KeyError as e:
            logger.warning(f"âš ï¸ Promptæ¨¡ç‰ˆå‚æ•°ç¼ºå¤±: {e}")
            # ä¸è¦è¿”å›åŸå§‹æ¨¡æ¿ï¼ç”¨å®‰å…¨é»˜è®¤å€¼å¡«å……ç¼ºå¤±å‚æ•°
            safe_kwargs = {
                'title': kwargs.get('title', ''),
                'description': kwargs.get('description', ''),
                'description_text': kwargs.get('description_text', ''),
                'author': kwargs.get('author', ''),
                'platform': kwargs.get('platform', ''),
                'feed_title': kwargs.get('feed_title', '')
            }
            return template.format(**safe_kwargs)
    
    def get_service_status(self) -> Dict[str, bool]:
        """
        è·å–æœåŠ¡çŠ¶æ€
        
        Returns:
            Dict[str, bool]: å„æœåŠ¡çš„å¯ç”¨çŠ¶æ€
        """
        return {
            "llm_available": self.llm_service is not None,
            "vector_available": self.vector_service is not None,
            "prompt_loaded": len(self.prompt_templates) > 0
        }
    
    def is_llm_available(self) -> bool:
        """æ£€æŸ¥LLMæœåŠ¡æ˜¯å¦å¯ç”¨"""
        return self.llm_service is not None
    
    def is_vector_available(self) -> bool:
        """æ£€æŸ¥å‘é‡æœåŠ¡æ˜¯å¦å¯ç”¨"""
        return self.vector_service is not None


# åˆ›å»ºå…¨å±€å®ä¾‹
ai_service_manager = AIServiceManager() 